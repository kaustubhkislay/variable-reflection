{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Reflection Experiment Runner\n",
    "\n",
    "This notebook allows you to run experiments with configurable parameters:\n",
    "- **Benchmarks**: ETHICS, MoralChoice, MORABLES\n",
    "- **Reflection Levels**: 0-5\n",
    "- **Thinking Conditions**: ON/OFF\n",
    "- **Sample Size**: Number of items per benchmark\n",
    "- **Number of Runs**: For consistency analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load environment variables\n",
    "with open('.env', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#'):\n",
    "            match = re.match(r'(\\w+)\\s*=\\s*[\"\\']?([^\"\\']+)[\"\\']?', line)\n",
    "            if match:\n",
    "                os.environ[match.group(1)] = match.group(2)\n",
    "\n",
    "# Import project modules\n",
    "from prompts import get_ethics_prompt, get_moralchoice_prompt, get_morables_prompt\n",
    "from src.api import call_with_rate_limit\n",
    "from src.extraction import (\n",
    "    extract_ethics_answer,\n",
    "    extract_moralchoice_with_confidence,\n",
    "    extract_morables_answer,\n",
    "    extract_confidence_score,\n",
    "    categorize_confidence,\n",
    "    count_reasoning_markers,\n",
    "    count_uncertainty_markers\n",
    ")\n",
    "import config\n",
    "\n",
    "print(f\"Model: {config.MODEL}\")\n",
    "print(f\"API configured: {'ANTHROPIC_API_KEY' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration\n",
    "\n",
    "**Modify these parameters to customize your experiment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT PARAMETERS - MODIFY THESE\n",
    "# =============================================================================\n",
    "\n",
    "# Which benchmarks to run (set to True/False)\n",
    "RUN_ETHICS = True\n",
    "RUN_MORALCHOICE = True\n",
    "RUN_MORABLES = True\n",
    "\n",
    "# Reflection levels to test (0-5)\n",
    "# Level 0: Direct, Level 1: Minimal, Level 2: CoT, Level 3: Structured\n",
    "# Level 4: Adversarial, Level 5: Two-Pass (2 API calls per item)\n",
    "LEVELS = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "# Thinking conditions\n",
    "THINKING_CONDITIONS = [False, True]  # [OFF, ON]\n",
    "\n",
    "# Sample size per benchmark (set to None for full dataset)\n",
    "SAMPLE_SIZE = 10  # Start small for testing, increase for full experiment\n",
    "\n",
    "# Number of runs (for consistency analysis)\n",
    "N_RUNS = 1\n",
    "\n",
    "# Whether to include confidence scoring\n",
    "INCLUDE_CONFIDENCE = True\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"results/notebook_runs\"\n",
    "\n",
    "# =============================================================================\n",
    "# Calculate estimated API calls\n",
    "# =============================================================================\n",
    "def estimate_api_calls():\n",
    "    calls_per_item = len(LEVELS) * len(THINKING_CONDITIONS) * N_RUNS\n",
    "    # Level 5 requires 2 calls per item, others require 1\n",
    "    level_5_extra = (1 if 5 in LEVELS else 0) * len(THINKING_CONDITIONS) * N_RUNS\n",
    "    \n",
    "    total = 0\n",
    "    if RUN_ETHICS:\n",
    "        total += SAMPLE_SIZE * (calls_per_item + level_5_extra)\n",
    "    if RUN_MORALCHOICE:\n",
    "        total += SAMPLE_SIZE * (calls_per_item + level_5_extra)\n",
    "    if RUN_MORABLES:\n",
    "        total += SAMPLE_SIZE * (calls_per_item + level_5_extra)\n",
    "    return total\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Benchmarks: {', '.join([b for b, run in [('ETHICS', RUN_ETHICS), ('MoralChoice', RUN_MORALCHOICE), ('MORABLES', RUN_MORABLES)] if run])}\")\n",
    "print(f\"Levels: {LEVELS}\")\n",
    "print(f\"Thinking conditions: {['OFF' if not t else 'ON' for t in THINKING_CONDITIONS]}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE}\")\n",
    "print(f\"Runs: {N_RUNS}\")\n",
    "print(f\"Include confidence: {INCLUDE_CONFIDENCE}\")\n",
    "print(f\"\\nEstimated API calls: ~{estimate_api_calls()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load datasets\n",
    "datasets = {}\n",
    "\n",
    "if RUN_ETHICS:\n",
    "    ethics_df = pd.read_csv(\"data/ethics_sample.csv\")\n",
    "    if SAMPLE_SIZE and len(ethics_df) > SAMPLE_SIZE:\n",
    "        # Balance by subscale\n",
    "        per_subscale = SAMPLE_SIZE // 3\n",
    "        subscales = []\n",
    "        for subscale in ['commonsense', 'deontology', 'virtue']:\n",
    "            subset = ethics_df[ethics_df['subscale'] == subscale].head(per_subscale)\n",
    "            subscales.append(subset)\n",
    "        ethics_df = pd.concat(subscales, ignore_index=True)\n",
    "    datasets['ethics'] = ethics_df\n",
    "    print(f\"ETHICS: {len(ethics_df)} items\")\n",
    "    print(f\"  Subscales: {ethics_df['subscale'].value_counts().to_dict()}\")\n",
    "\n",
    "if RUN_MORALCHOICE:\n",
    "    mc_df = pd.read_csv(\"data/moralchoice_sample.csv\")\n",
    "    if SAMPLE_SIZE and len(mc_df) > SAMPLE_SIZE:\n",
    "        # Balance by ambiguity\n",
    "        per_level = SAMPLE_SIZE // 2\n",
    "        low_amb = mc_df[mc_df['ambiguity'] == 'low'].head(per_level)\n",
    "        high_amb = mc_df[mc_df['ambiguity'] == 'high'].head(per_level)\n",
    "        mc_df = pd.concat([low_amb, high_amb], ignore_index=True)\n",
    "    datasets['moralchoice'] = mc_df\n",
    "    print(f\"\\nMoralChoice: {len(mc_df)} items\")\n",
    "    if 'ambiguity' in mc_df.columns:\n",
    "        print(f\"  Ambiguity: {mc_df['ambiguity'].value_counts().to_dict()}\")\n",
    "\n",
    "if RUN_MORABLES:\n",
    "    morables_path = \"data/morables/morables_sample.csv\"\n",
    "    if Path(morables_path).exists():\n",
    "        morables_df = pd.read_csv(morables_path)\n",
    "        if SAMPLE_SIZE and len(morables_df) > SAMPLE_SIZE:\n",
    "            morables_df = morables_df.sample(n=SAMPLE_SIZE, random_state=config.RANDOM_SEED)\n",
    "        datasets['morables'] = morables_df\n",
    "        print(f\"\\nMORABLES: {len(morables_df)} items\")\n",
    "    else:\n",
    "        print(f\"\\nMORABLES: Not found at {morables_path}\")\n",
    "        print(\"  Run 'python prepare_data.py' to download the dataset\")\n",
    "        RUN_MORABLES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment Runner Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_ethics(row, level, thinking, include_confidence=True):\n",
    "    \"\"\"Run single ETHICS item.\"\"\"\n",
    "    if level == 5:\n",
    "        prompt1 = get_ethics_prompt(5, row['scenario'], include_confidence=include_confidence)\n",
    "        response1 = call_with_rate_limit(prompt1, thinking)\n",
    "        \n",
    "        prompt2 = get_ethics_prompt(5, row['scenario'], response1.content, include_confidence=include_confidence)\n",
    "        response2 = call_with_rate_limit(prompt2, thinking)\n",
    "        \n",
    "        return {\n",
    "            'content': response2.content,\n",
    "            'thinking': response2.thinking,\n",
    "            'full_response': f\"[PASS1]\\n{response1.content}\\n\\n[PASS2]\\n{response2.content}\",\n",
    "            'input_tokens': response1.input_tokens + response2.input_tokens,\n",
    "            'output_tokens': response1.output_tokens + response2.output_tokens,\n",
    "        }\n",
    "    else:\n",
    "        prompt = get_ethics_prompt(level, row['scenario'], include_confidence=include_confidence)\n",
    "        response = call_with_rate_limit(prompt, thinking)\n",
    "        return {\n",
    "            'content': response.content,\n",
    "            'thinking': response.thinking,\n",
    "            'full_response': response.content,\n",
    "            'input_tokens': response.input_tokens,\n",
    "            'output_tokens': response.output_tokens,\n",
    "        }\n",
    "\n",
    "\n",
    "def run_single_moralchoice(row, level, thinking, include_confidence=True):\n",
    "    \"\"\"Run single MoralChoice item.\"\"\"\n",
    "    context = row.get('context', '')\n",
    "    \n",
    "    if level == 5:\n",
    "        prompt1 = get_moralchoice_prompt(5, context, row['option_a'], row['option_b'],\n",
    "                                         include_confidence=include_confidence)\n",
    "        response1 = call_with_rate_limit(prompt1, thinking)\n",
    "        \n",
    "        prompt2 = get_moralchoice_prompt(5, context, row['option_a'], row['option_b'],\n",
    "                                         previous_response=response1.content,\n",
    "                                         include_confidence=include_confidence)\n",
    "        response2 = call_with_rate_limit(prompt2, thinking)\n",
    "        \n",
    "        return {\n",
    "            'content': response2.content,\n",
    "            'thinking': response2.thinking,\n",
    "            'full_response': f\"[PASS1]\\n{response1.content}\\n\\n[PASS2]\\n{response2.content}\",\n",
    "            'input_tokens': response1.input_tokens + response2.input_tokens,\n",
    "            'output_tokens': response1.output_tokens + response2.output_tokens,\n",
    "        }\n",
    "    else:\n",
    "        prompt = get_moralchoice_prompt(level, context, row['option_a'], row['option_b'],\n",
    "                                        include_confidence=include_confidence)\n",
    "        response = call_with_rate_limit(prompt, thinking)\n",
    "        return {\n",
    "            'content': response.content,\n",
    "            'thinking': response.thinking,\n",
    "            'full_response': response.content,\n",
    "            'input_tokens': response.input_tokens,\n",
    "            'output_tokens': response.output_tokens,\n",
    "        }\n",
    "\n",
    "\n",
    "def run_single_morables(row, level, thinking, include_confidence=True):\n",
    "    \"\"\"Run single MORABLES item.\"\"\"\n",
    "    options = [row['option_a'], row['option_b'], row['option_c'], row['option_d'], row['option_e']]\n",
    "    \n",
    "    if level == 5:\n",
    "        prompt1 = get_morables_prompt(5, row['fable'], options, include_confidence=include_confidence)\n",
    "        response1 = call_with_rate_limit(prompt1, thinking)\n",
    "        \n",
    "        prompt2 = get_morables_prompt(5, row['fable'], options,\n",
    "                                       previous_response=response1.content,\n",
    "                                       include_confidence=include_confidence)\n",
    "        response2 = call_with_rate_limit(prompt2, thinking)\n",
    "        \n",
    "        return {\n",
    "            'content': response2.content,\n",
    "            'thinking': response2.thinking,\n",
    "            'full_response': f\"[PASS1]\\n{response1.content}\\n\\n[PASS2]\\n{response2.content}\",\n",
    "            'input_tokens': response1.input_tokens + response2.input_tokens,\n",
    "            'output_tokens': response1.output_tokens + response2.output_tokens,\n",
    "        }\n",
    "    else:\n",
    "        prompt = get_morables_prompt(level, row['fable'], options, include_confidence=include_confidence)\n",
    "        response = call_with_rate_limit(prompt, thinking)\n",
    "        return {\n",
    "            'content': response.content,\n",
    "            'thinking': response.thinking,\n",
    "            'full_response': response.content,\n",
    "            'input_tokens': response.input_tokens,\n",
    "            'output_tokens': response.output_tokens,\n",
    "        }\n",
    "\n",
    "print(\"Runner functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run ETHICS Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_ETHICS:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RUNNING ETHICS EXPERIMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    ethics_results = []\n",
    "    ethics_df = datasets['ethics']\n",
    "    \n",
    "    total_conditions = len(LEVELS) * len(THINKING_CONDITIONS) * N_RUNS\n",
    "    condition_num = 0\n",
    "    \n",
    "    for run in range(N_RUNS):\n",
    "        for thinking in THINKING_CONDITIONS:\n",
    "            for level in LEVELS:\n",
    "                condition_num += 1\n",
    "                thinking_label = \"ON\" if thinking else \"OFF\"\n",
    "                \n",
    "                print(f\"\\n[{condition_num}/{total_conditions}] Run {run+1}, Level {level}, Thinking {thinking_label}\")\n",
    "                \n",
    "                for _, row in tqdm(ethics_df.iterrows(), total=len(ethics_df), \n",
    "                                   desc=f\"L{level}-{thinking_label}\"):\n",
    "                    try:\n",
    "                        response_data = run_single_ethics(row, level, thinking, INCLUDE_CONFIDENCE)\n",
    "                        \n",
    "                        extracted = extract_ethics_answer(response_data['content'])\n",
    "                        confidence = extract_confidence_score(response_data['content']) if INCLUDE_CONFIDENCE else None\n",
    "                        \n",
    "                        ethics_results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'subscale': row['subscale'],\n",
    "                            'scenario': row['scenario'][:200],\n",
    "                            'correct_answer': row['label'],\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'response': response_data['full_response'],\n",
    "                            'thinking_content': response_data['thinking'],\n",
    "                            'extracted_answer': extracted,\n",
    "                            'correct': extracted == row['label'] if extracted else None,\n",
    "                            'confidence': confidence,\n",
    "                            'confidence_category': categorize_confidence(confidence),\n",
    "                            'response_length': len(response_data['full_response'].split()),\n",
    "                            'reasoning_markers': count_reasoning_markers(response_data['full_response']),\n",
    "                            'uncertainty_markers': count_uncertainty_markers(response_data['full_response']),\n",
    "                            'input_tokens': response_data['input_tokens'],\n",
    "                            'output_tokens': response_data['output_tokens'],\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error on {row['item_id']}: {e}\")\n",
    "                        ethics_results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'error': str(e),\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                        })\n",
    "    \n",
    "    # Save results\n",
    "    ethics_results_df = pd.DataFrame(ethics_results)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    ethics_results_df.to_csv(f\"{OUTPUT_DIR}/ethics_results_{timestamp}.csv\", index=False)\n",
    "    print(f\"\\nETHICS complete: {len(ethics_results_df)} observations\")\n",
    "    print(f\"Saved to: {OUTPUT_DIR}/ethics_results_{timestamp}.csv\")\n",
    "else:\n",
    "    print(\"Skipping ETHICS (RUN_ETHICS = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run MoralChoice Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MORALCHOICE:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RUNNING MORALCHOICE EXPERIMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    mc_results = []\n",
    "    mc_df = datasets['moralchoice']\n",
    "    \n",
    "    total_conditions = len(LEVELS) * len(THINKING_CONDITIONS) * N_RUNS\n",
    "    condition_num = 0\n",
    "    \n",
    "    for run in range(N_RUNS):\n",
    "        for thinking in THINKING_CONDITIONS:\n",
    "            for level in LEVELS:\n",
    "                condition_num += 1\n",
    "                thinking_label = \"ON\" if thinking else \"OFF\"\n",
    "                \n",
    "                print(f\"\\n[{condition_num}/{total_conditions}] Run {run+1}, Level {level}, Thinking {thinking_label}\")\n",
    "                \n",
    "                for _, row in tqdm(mc_df.iterrows(), total=len(mc_df),\n",
    "                                   desc=f\"L{level}-{thinking_label}\"):\n",
    "                    try:\n",
    "                        response_data = run_single_moralchoice(row, level, thinking, INCLUDE_CONFIDENCE)\n",
    "                        \n",
    "                        extraction = extract_moralchoice_with_confidence(response_data['content'])\n",
    "                        \n",
    "                        mc_results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'context': row['context'][:100],\n",
    "                            'option_a': row['option_a'][:100],\n",
    "                            'option_b': row['option_b'][:100],\n",
    "                            'ambiguity': row.get('ambiguity', None),\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'response': response_data['full_response'],\n",
    "                            'thinking_content': response_data['thinking'],\n",
    "                            'extracted_answer': extraction['answer'],\n",
    "                            'confidence': extraction['confidence'],\n",
    "                            'confidence_category': extraction['confidence_category'],\n",
    "                            'response_length': len(response_data['full_response'].split()),\n",
    "                            'reasoning_markers': count_reasoning_markers(response_data['full_response']),\n",
    "                            'uncertainty_markers': count_uncertainty_markers(response_data['full_response']),\n",
    "                            'input_tokens': response_data['input_tokens'],\n",
    "                            'output_tokens': response_data['output_tokens'],\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error on {row['item_id']}: {e}\")\n",
    "                        mc_results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'error': str(e),\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                        })\n",
    "    \n",
    "    # Save results\n",
    "    mc_results_df = pd.DataFrame(mc_results)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mc_results_df.to_csv(f\"{OUTPUT_DIR}/moralchoice_results_{timestamp}.csv\", index=False)\n",
    "    print(f\"\\nMoralChoice complete: {len(mc_results_df)} observations\")\n",
    "    print(f\"Saved to: {OUTPUT_DIR}/moralchoice_results_{timestamp}.csv\")\n",
    "else:\n",
    "    print(\"Skipping MoralChoice (RUN_MORALCHOICE = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run MORABLES Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MORABLES:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RUNNING MORABLES EXPERIMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    morables_results = []\n",
    "    morables_df = datasets['morables']\n",
    "    \n",
    "    total_conditions = len(LEVELS) * len(THINKING_CONDITIONS) * N_RUNS\n",
    "    condition_num = 0\n",
    "    \n",
    "    for run in range(N_RUNS):\n",
    "        for thinking in THINKING_CONDITIONS:\n",
    "            for level in LEVELS:\n",
    "                condition_num += 1\n",
    "                thinking_label = \"ON\" if thinking else \"OFF\"\n",
    "                \n",
    "                print(f\"\\n[{condition_num}/{total_conditions}] Run {run+1}, Level {level}, Thinking {thinking_label}\")\n",
    "                \n",
    "                for _, row in tqdm(morables_df.iterrows(), total=len(morables_df),\n",
    "                                   desc=f\"L{level}-{thinking_label}\"):\n",
    "                    try:\n",
    "                        response_data = run_single_morables(row, level, thinking, INCLUDE_CONFIDENCE)\n",
    "                        \n",
    "                        extracted = extract_morables_answer(response_data['content'])\n",
    "                        confidence = extract_confidence_score(response_data['content']) if INCLUDE_CONFIDENCE else None\n",
    "                        \n",
    "                        # Map letter to index for correctness check\n",
    "                        letter_to_idx = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4}\n",
    "                        extracted_idx = letter_to_idx.get(extracted)\n",
    "                        correct = extracted_idx == row['correct_idx'] if extracted_idx is not None else None\n",
    "                        \n",
    "                        morables_results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'fable': row['fable'][:200],\n",
    "                            'correct_idx': row['correct_idx'],\n",
    "                            'correct_answer': ['A', 'B', 'C', 'D', 'E'][row['correct_idx']],\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'response': response_data['full_response'],\n",
    "                            'thinking_content': response_data['thinking'],\n",
    "                            'extracted_answer': extracted,\n",
    "                            'correct': correct,\n",
    "                            'confidence': confidence,\n",
    "                            'confidence_category': categorize_confidence(confidence),\n",
    "                            'response_length': len(response_data['full_response'].split()),\n",
    "                            'reasoning_markers': count_reasoning_markers(response_data['full_response']),\n",
    "                            'uncertainty_markers': count_uncertainty_markers(response_data['full_response']),\n",
    "                            'input_tokens': response_data['input_tokens'],\n",
    "                            'output_tokens': response_data['output_tokens'],\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error on {row['item_id']}: {e}\")\n",
    "                        morables_results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'error': str(e),\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                        })\n",
    "    \n",
    "    # Save results\n",
    "    morables_results_df = pd.DataFrame(morables_results)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    morables_results_df.to_csv(f\"{OUTPUT_DIR}/morables_results_{timestamp}.csv\", index=False)\n",
    "    print(f\"\\nMORABLES complete: {len(morables_results_df)} observations\")\n",
    "    print(f\"Saved to: {OUTPUT_DIR}/morables_results_{timestamp}.csv\")\n",
    "else:\n",
    "    print(\"Skipping MORABLES (RUN_MORABLES = False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(df, name):\n",
    "    \"\"\"Print quick summary of experiment results.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name} RESULTS SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Filter to valid results\n",
    "    valid = df[df['correct'].notna()]\n",
    "    errors = df[df.get('error', pd.Series()).notna()] if 'error' in df.columns else pd.DataFrame()\n",
    "    \n",
    "    print(f\"Total observations: {len(df)}\")\n",
    "    print(f\"Valid extractions: {len(valid)} ({len(valid)/len(df)*100:.1f}%)\")\n",
    "    if len(errors) > 0:\n",
    "        print(f\"Errors: {len(errors)}\")\n",
    "    \n",
    "    if len(valid) > 0:\n",
    "        print(f\"\\nOverall accuracy: {valid['correct'].mean():.3f}\")\n",
    "        \n",
    "        # By level\n",
    "        print(\"\\nAccuracy by level:\")\n",
    "        for level in sorted(valid['level'].unique()):\n",
    "            level_data = valid[valid['level'] == level]\n",
    "            print(f\"  Level {level}: {level_data['correct'].mean():.3f} (n={len(level_data)})\")\n",
    "        \n",
    "        # By thinking condition\n",
    "        print(\"\\nAccuracy by thinking condition:\")\n",
    "        for thinking in [False, True]:\n",
    "            think_data = valid[valid['thinking'] == thinking]\n",
    "            label = \"ON\" if thinking else \"OFF\"\n",
    "            if len(think_data) > 0:\n",
    "                print(f\"  Thinking {label}: {think_data['correct'].mean():.3f} (n={len(think_data)})\")\n",
    "\n",
    "# Summarize each benchmark\n",
    "if RUN_ETHICS and 'ethics_results_df' in dir():\n",
    "    summarize_results(ethics_results_df, \"ETHICS\")\n",
    "\n",
    "if RUN_MORALCHOICE and 'mc_results_df' in dir():\n",
    "    # MoralChoice doesn't have \"correct\" - show extraction rate\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"MORALCHOICE RESULTS SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total observations: {len(mc_results_df)}\")\n",
    "    valid = mc_results_df[mc_results_df['extracted_answer'].notna()]\n",
    "    print(f\"Valid extractions: {len(valid)} ({len(valid)/len(mc_results_df)*100:.1f}%)\")\n",
    "    print(f\"\\nAnswer distribution: {valid['extracted_answer'].value_counts().to_dict()}\")\n",
    "    if 'confidence' in valid.columns:\n",
    "        print(f\"Mean confidence: {valid['confidence'].mean():.1f}\")\n",
    "\n",
    "if RUN_MORABLES and 'morables_results_df' in dir():\n",
    "    summarize_results(morables_results_df, \"MORABLES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Analysis (Optional)\n",
    "\n",
    "Use this cell to explore your results interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: View a specific response\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# if 'ethics_results_df' in dir():\n",
    "#     # Show first response for Level 5 with thinking ON\n",
    "#     sample = ethics_results_df[(ethics_results_df['level'] == 5) & (ethics_results_df['thinking'] == True)].iloc[0]\n",
    "#     print(f\"Item: {sample['item_id']}\")\n",
    "#     print(f\"Scenario: {sample['scenario']}\")\n",
    "#     print(f\"Correct: {sample['correct_answer']}\")\n",
    "#     print(f\"Extracted: {sample['extracted_answer']}\")\n",
    "#     print(f\"\\nResponse:\\n{sample['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Pivot table of accuracy by level and thinking\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# if 'ethics_results_df' in dir():\n",
    "#     valid = ethics_results_df[ethics_results_df['correct'].notna()]\n",
    "#     pivot = valid.pivot_table(values='correct', index='level', columns='thinking', aggfunc='mean')\n",
    "#     pivot.columns = ['Thinking OFF', 'Thinking ON']\n",
    "#     display(pivot.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference: Parameter Options\n",
    "\n",
    "| Parameter | Options | Description |\n",
    "|-----------|---------|-------------|\n",
    "| `RUN_ETHICS` | `True`/`False` | Run ETHICS benchmark |\n",
    "| `RUN_MORALCHOICE` | `True`/`False` | Run MoralChoice benchmark |\n",
    "| `RUN_MORABLES` | `True`/`False` | Run MORABLES benchmark |\n",
    "| `LEVELS` | `[0,1,2,3,4,5]` | Which reflection levels to test |\n",
    "| `THINKING_CONDITIONS` | `[False, True]` | OFF and/or ON |\n",
    "| `SAMPLE_SIZE` | `int` or `None` | Items per benchmark (None = all) |\n",
    "| `N_RUNS` | `int` | Number of runs for consistency |\n",
    "| `INCLUDE_CONFIDENCE` | `True`/`False` | Ask for confidence scores |\n",
    "\n",
    "### Level Descriptions\n",
    "- **Level 0**: Direct answer only\n",
    "- **Level 1**: Minimal instruction\n",
    "- **Level 2**: Chain-of-Thought (step-by-step)\n",
    "- **Level 3**: Structured analysis\n",
    "- **Level 4**: Adversarial (consider counterarguments)\n",
    "- **Level 5**: Two-Pass reflection (2 API calls per item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
