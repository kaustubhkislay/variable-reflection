{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Reflection Experiment\n",
    "\n",
    "This notebook allows you to run moral reasoning experiments with configurable parameters.\n",
    "\n",
    "**Modify the parameters in the cells below to customize your experiment.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, Markdown, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project imports\n",
    "import config\n",
    "from prompts import get_ethics_prompt, get_moralchoice_prompt, ETHICS_PROMPTS, MORALCHOICE_PROMPTS\n",
    "from src.api import call_with_rate_limit\n",
    "from src.extraction import (\n",
    "    extract_ethics_answer, \n",
    "    extract_moralchoice_answer,\n",
    "    count_reasoning_markers,\n",
    "    count_uncertainty_markers\n",
    ")\n",
    "\n",
    "# Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(f\"Model: {config.MODEL}\")\n",
    "print(f\"API Key loaded: {'Yes' if config.ANTHROPIC_API_KEY else 'No'}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Experiment Parameters\n",
    "\n",
    "### **Modify these values to customize your experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# EXPERIMENT PARAMETERS - MODIFY THESE\n# ============================================================\n\n# Which benchmark to run\nBENCHMARK = \"moralchoice\"  # Options: \"ethics\", \"moralchoice\", \"both\"\n\n# Number of items to test (set to None for all items)\nN_ITEMS = 10\n\n# Which reflection levels to test (0-5)\nLEVELS = [1]  # Options: any subset of [0, 1, 2, 3, 4, 5]\n\n# Extended thinking conditions\nTHINKING_CONDITIONS = [False, True]  # Options: [False], [True], or [False, True]\n\n# Number of runs per condition (for consistency analysis)\nN_RUNS = 1  # Increase for consistency measurement\n\n# Random seed for sampling\nRANDOM_SEED = 42\n\n# ============================================================\n# DISPLAY CONFIGURATION\n# ============================================================\nprint(\"Current Configuration:\")\nprint(f\"  Benchmark: {BENCHMARK}\")\nprint(f\"  Items: {N_ITEMS if N_ITEMS else 'All'}\")\nprint(f\"  Levels: {LEVELS}\")\nprint(f\"  Thinking: {['OFF' if not t else 'ON' for t in THINKING_CONDITIONS]}\")\nprint(f\"  Runs: {N_RUNS}\")\n\n# Calculate total API calls (clearer formula)\nn_items_ethics = N_ITEMS if N_ITEMS else len(pd.read_csv('../data/ethics_sample.csv'))\nn_items_mc = N_ITEMS if N_ITEMS else len(pd.read_csv('../data/moralchoice_sample.csv'))\n\n# Count calls: each non-level-5 = 1 call, level 5 = 2 calls\ncalls_per_item_per_condition = sum(2 if level == 5 else 1 for level in LEVELS)\ncalls_per_item = calls_per_item_per_condition * len(THINKING_CONDITIONS) * N_RUNS\n\nif BENCHMARK == \"ethics\":\n    total_calls = n_items_ethics * calls_per_item\n    print(f\"\\nCalculation: {n_items_ethics} items × {len(LEVELS)} levels × {len(THINKING_CONDITIONS)} thinking × {N_RUNS} runs\")\nelif BENCHMARK == \"moralchoice\":\n    total_calls = n_items_mc * calls_per_item\n    print(f\"\\nCalculation: {n_items_mc} items × {len(LEVELS)} levels × {len(THINKING_CONDITIONS)} thinking × {N_RUNS} runs\")\nelse:  # both\n    total_calls = (n_items_ethics + n_items_mc) * calls_per_item\n    print(f\"\\nCalculation: ({n_items_ethics} + {n_items_mc}) items × {len(LEVELS)} levels × {len(THINKING_CONDITIONS)} thinking × {N_RUNS} runs\")\n\n# Add note about level 5\nif 5 in LEVELS:\n    print(\"  (Level 5 requires 2 API calls per item)\")\n\nprint(f\"\\nEstimated API calls: {total_calls}\")\nprint(f\"Estimated time: ~{total_calls * 1.2 / 60:.1f} minutes (at 50 calls/min)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "ethics_data = pd.read_csv('../data/ethics_sample.csv')\n",
    "mc_data = pd.read_csv('../data/moralchoice_sample.csv')\n",
    "\n",
    "# Sample if N_ITEMS specified\n",
    "if N_ITEMS:\n",
    "    ethics_sample = ethics_data.sample(n=min(N_ITEMS, len(ethics_data)), random_state=RANDOM_SEED)\n",
    "    mc_sample = mc_data.sample(n=min(N_ITEMS, len(mc_data)), random_state=RANDOM_SEED)\n",
    "else:\n",
    "    ethics_sample = ethics_data\n",
    "    mc_sample = mc_data\n",
    "\n",
    "print(f\"ETHICS sample: {len(ethics_sample)} items\")\n",
    "print(f\"MoralChoice sample: {len(mc_sample)} items\")\n",
    "\n",
    "# Preview\n",
    "if BENCHMARK in [\"ethics\", \"both\"]:\n",
    "    print(\"\\nETHICS preview:\")\n",
    "    display(ethics_sample[['item_id', 'subscale', 'scenario', 'label']].head(3))\n",
    "\n",
    "if BENCHMARK in [\"moralchoice\", \"both\"]:\n",
    "    print(\"\\nMoralChoice preview:\")\n",
    "    display(mc_sample[['item_id', 'context', 'option_a', 'option_b', 'ambiguity']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Preview Prompts\n",
    "\n",
    "See what prompts will be sent for each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview prompts for selected levels\n",
    "sample_scenario = ethics_sample.iloc[0]['scenario'][:200] + \"...\"\n",
    "sample_context = mc_sample.iloc[0]['context'] if 'context' in mc_sample.columns else \"\"\n",
    "sample_option_a = mc_sample.iloc[0]['option_a'][:100] + \"...\"\n",
    "sample_option_b = mc_sample.iloc[0]['option_b'][:100] + \"...\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PROMPT PREVIEWS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for level in LEVELS:\n",
    "    print(f\"\\n{'─' * 60}\")\n",
    "    print(f\"LEVEL {level}\")\n",
    "    print(f\"{'─' * 60}\")\n",
    "    \n",
    "    if BENCHMARK in [\"ethics\", \"both\"]:\n",
    "        prompt = get_ethics_prompt(level, sample_scenario)\n",
    "        print(f\"\\n[ETHICS Prompt]:\\n{prompt[:500]}{'...' if len(prompt) > 500 else ''}\")\n",
    "    \n",
    "    if BENCHMARK in [\"moralchoice\", \"both\"]:\n",
    "        prompt = get_moralchoice_prompt(level, sample_context, sample_option_a, sample_option_b)\n",
    "        print(f\"\\n[MoralChoice Prompt]:\\n{prompt[:500]}{'...' if len(prompt) > 500 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run Experiment\n",
    "\n",
    "Execute the experiment with your configured parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ethics_item(row, level, thinking):\n",
    "    \"\"\"Run single ETHICS item.\"\"\"\n",
    "    if level == 5:\n",
    "        # Two-pass\n",
    "        prompt1 = get_ethics_prompt(5, row['scenario'])\n",
    "        response1 = call_with_rate_limit(prompt1, thinking)\n",
    "        \n",
    "        prompt2 = get_ethics_prompt(5, row['scenario'], response1.content)\n",
    "        response2 = call_with_rate_limit(prompt2, thinking)\n",
    "        \n",
    "        return {\n",
    "            'response': f\"[PASS1]\\n{response1.content}\\n\\n[PASS2]\\n{response2.content}\",\n",
    "            'thinking_content': response2.thinking,\n",
    "            'input_tokens': response1.input_tokens + response2.input_tokens,\n",
    "            'output_tokens': response1.output_tokens + response2.output_tokens,\n",
    "            'content_for_extraction': response2.content\n",
    "        }\n",
    "    else:\n",
    "        prompt = get_ethics_prompt(level, row['scenario'])\n",
    "        response = call_with_rate_limit(prompt, thinking)\n",
    "        return {\n",
    "            'response': response.content,\n",
    "            'thinking_content': response.thinking,\n",
    "            'input_tokens': response.input_tokens,\n",
    "            'output_tokens': response.output_tokens,\n",
    "            'content_for_extraction': response.content\n",
    "        }\n",
    "\n",
    "\n",
    "def run_moralchoice_item(row, level, thinking):\n",
    "    \"\"\"Run single MoralChoice item.\"\"\"\n",
    "    context = row.get('context', '')\n",
    "    \n",
    "    if level == 5:\n",
    "        prompt1 = get_moralchoice_prompt(5, context, row['option_a'], row['option_b'])\n",
    "        response1 = call_with_rate_limit(prompt1, thinking)\n",
    "        \n",
    "        prompt2 = get_moralchoice_prompt(5, context, row['option_a'], row['option_b'], response1.content)\n",
    "        response2 = call_with_rate_limit(prompt2, thinking)\n",
    "        \n",
    "        return {\n",
    "            'response': f\"[PASS1]\\n{response1.content}\\n\\n[PASS2]\\n{response2.content}\",\n",
    "            'thinking_content': response2.thinking,\n",
    "            'input_tokens': response1.input_tokens + response2.input_tokens,\n",
    "            'output_tokens': response1.output_tokens + response2.output_tokens,\n",
    "            'content_for_extraction': response2.content\n",
    "        }\n",
    "    else:\n",
    "        prompt = get_moralchoice_prompt(level, context, row['option_a'], row['option_b'])\n",
    "        response = call_with_rate_limit(prompt, thinking)\n",
    "        return {\n",
    "            'response': response.content,\n",
    "            'thinking_content': response.thinking,\n",
    "            'input_tokens': response.input_tokens,\n",
    "            'output_tokens': response.output_tokens,\n",
    "            'content_for_extraction': response.content\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"Experiment runner functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN EXPERIMENT\n",
    "# ============================================================\n",
    "\n",
    "ethics_results = []\n",
    "mc_results = []\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"Starting experiment at {start_time.strftime('%H:%M:%S')}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run ETHICS\n",
    "if BENCHMARK in [\"ethics\", \"both\"]:\n",
    "    print(\"\\nRunning ETHICS benchmark...\")\n",
    "    \n",
    "    for run in range(N_RUNS):\n",
    "        for thinking in THINKING_CONDITIONS:\n",
    "            thinking_label = \"ON\" if thinking else \"OFF\"\n",
    "            \n",
    "            for level in LEVELS:\n",
    "                desc = f\"Run {run+1}, Level {level}, Thinking {thinking_label}\"\n",
    "                \n",
    "                for _, row in tqdm(ethics_sample.iterrows(), total=len(ethics_sample), desc=desc):\n",
    "                    try:\n",
    "                        result = run_ethics_item(row, level, thinking)\n",
    "                        extracted = extract_ethics_answer(result['content_for_extraction'])\n",
    "                        \n",
    "                        ethics_results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'subscale': row['subscale'],\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'response': result['response'],\n",
    "                            'thinking_content': result['thinking_content'],\n",
    "                            'extracted_answer': extracted,\n",
    "                            'correct_answer': row['label'],\n",
    "                            'correct': extracted == row['label'] if extracted else None,\n",
    "                            'reasoning_markers': count_reasoning_markers(result['response']),\n",
    "                            'uncertainty_markers': count_uncertainty_markers(result['response']),\n",
    "                            'input_tokens': result['input_tokens'],\n",
    "                            'output_tokens': result['output_tokens'],\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error on {row['item_id']}: {e}\")\n",
    "\n",
    "# Run MoralChoice\n",
    "if BENCHMARK in [\"moralchoice\", \"both\"]:\n",
    "    print(\"\\nRunning MoralChoice benchmark...\")\n",
    "    \n",
    "    for run in range(N_RUNS):\n",
    "        for thinking in THINKING_CONDITIONS:\n",
    "            thinking_label = \"ON\" if thinking else \"OFF\"\n",
    "            \n",
    "            for level in LEVELS:\n",
    "                desc = f\"Run {run+1}, Level {level}, Thinking {thinking_label}\"\n",
    "                \n",
    "                for _, row in tqdm(mc_sample.iterrows(), total=len(mc_sample), desc=desc):\n",
    "                    try:\n",
    "                        result = run_moralchoice_item(row, level, thinking)\n",
    "                        extracted = extract_moralchoice_answer(result['content_for_extraction'])\n",
    "                        \n",
    "                        mc_results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'ambiguity': row.get('ambiguity', 'unknown'),\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'response': result['response'],\n",
    "                            'thinking_content': result['thinking_content'],\n",
    "                            'extracted_answer': extracted,\n",
    "                            'reasoning_markers': count_reasoning_markers(result['response']),\n",
    "                            'uncertainty_markers': count_uncertainty_markers(result['response']),\n",
    "                            'input_tokens': result['input_tokens'],\n",
    "                            'output_tokens': result['output_tokens'],\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error on {row['item_id']}: {e}\")\n",
    "\n",
    "# Convert to DataFrames\n",
    "ethics_df = pd.DataFrame(ethics_results) if ethics_results else pd.DataFrame()\n",
    "mc_df = pd.DataFrame(mc_results) if mc_results else pd.DataFrame()\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Experiment complete! Duration: {duration}\")\n",
    "print(f\"ETHICS results: {len(ethics_df)} observations\")\n",
    "print(f\"MoralChoice results: {len(mc_df)} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ETHICS RESULTS SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "if len(ethics_df) > 0:\n",
    "    print(\"ETHICS ACCURACY BY CONDITION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    accuracy = ethics_df.groupby(['level', 'thinking']).agg({\n",
    "        'correct': ['mean', 'count'],\n",
    "        'extracted_answer': lambda x: x.isna().mean()\n",
    "    }).round(3)\n",
    "    accuracy.columns = ['accuracy', 'n', 'extraction_failure']\n",
    "    display(accuracy)\n",
    "    \n",
    "    # By subscale\n",
    "    print(\"\\nBy Subscale:\")\n",
    "    subscale_acc = ethics_df.groupby(['subscale', 'level', 'thinking'])['correct'].mean().round(3)\n",
    "    display(subscale_acc.unstack('thinking'))\n",
    "else:\n",
    "    print(\"No ETHICS results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MORALCHOICE RESULTS SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "if len(mc_df) > 0:\n",
    "    print(\"MORALCHOICE SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Extraction success\n",
    "    extraction = mc_df.groupby(['level', 'thinking']).agg({\n",
    "        'extracted_answer': lambda x: x.notna().mean()\n",
    "    }).round(3)\n",
    "    extraction.columns = ['extraction_success']\n",
    "    print(\"\\nExtraction Success Rate:\")\n",
    "    display(extraction)\n",
    "    \n",
    "    # Preference (% choosing A)\n",
    "    preference = mc_df.groupby(['level', 'thinking']).apply(\n",
    "        lambda x: (x['extracted_answer'] == 'A').sum() / x['extracted_answer'].notna().sum()\n",
    "        if x['extracted_answer'].notna().sum() > 0 else np.nan\n",
    "    ).round(3)\n",
    "    print(\"\\nPreference Rate (% choosing A):\")\n",
    "    display(preference.unstack('thinking'))\n",
    "else:\n",
    "    print(\"No MoralChoice results to display.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "COLORS = {'OFF': '#1f77b4', 'ON': '#ff7f0e'}\n",
    "\n",
    "if len(ethics_df) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax = axes[0]\n",
    "    for thinking in THINKING_CONDITIONS:\n",
    "        label = 'Thinking ON' if thinking else 'Thinking OFF'\n",
    "        color = COLORS['ON'] if thinking else COLORS['OFF']\n",
    "        \n",
    "        subset = ethics_df[ethics_df['thinking'] == thinking]\n",
    "        means = subset.groupby('level')['correct'].mean()\n",
    "        sems = subset.groupby('level')['correct'].sem()\n",
    "        \n",
    "        ax.errorbar(means.index, means.values, yerr=1.96*sems.values,\n",
    "                   label=label, color=color, marker='o', linewidth=2, capsize=5)\n",
    "    \n",
    "    ax.set_xlabel('Prompt Level')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('ETHICS Accuracy by Condition')\n",
    "    ax.set_xticks(LEVELS)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    \n",
    "    # Response length plot\n",
    "    ax = axes[1]\n",
    "    ethics_df['response_length'] = ethics_df['response'].str.split().str.len()\n",
    "    \n",
    "    for thinking in THINKING_CONDITIONS:\n",
    "        label = 'Thinking ON' if thinking else 'Thinking OFF'\n",
    "        color = COLORS['ON'] if thinking else COLORS['OFF']\n",
    "        \n",
    "        subset = ethics_df[ethics_df['thinking'] == thinking]\n",
    "        means = subset.groupby('level')['response_length'].mean()\n",
    "        \n",
    "        ax.plot(means.index, means.values, label=label, color=color, marker='s', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Prompt Level')\n",
    "    ax.set_ylabel('Response Length (words)')\n",
    "    ax.set_title('Response Length by Condition')\n",
    "    ax.set_xticks(LEVELS)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(mc_df) > 0 and len(THINKING_CONDITIONS) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    \n",
    "    for thinking in THINKING_CONDITIONS:\n",
    "        label = 'Thinking ON' if thinking else 'Thinking OFF'\n",
    "        color = COLORS['ON'] if thinking else COLORS['OFF']\n",
    "        \n",
    "        subset = mc_df[mc_df['thinking'] == thinking]\n",
    "        pref = subset.groupby('level').apply(\n",
    "            lambda x: (x['extracted_answer'] == 'A').sum() / x['extracted_answer'].notna().sum()\n",
    "            if x['extracted_answer'].notna().sum() > 0 else np.nan\n",
    "        )\n",
    "        \n",
    "        ax.plot(pref.index, pref.values, label=label, color=color, marker='o', linewidth=2)\n",
    "    \n",
    "    ax.axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Prompt Level')\n",
    "    ax.set_ylabel('Preference Rate (% choosing A)')\n",
    "    ax.set_title('MoralChoice Preference by Condition')\n",
    "    ax.set_xticks(LEVELS)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0.3, 0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Inspect Individual Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSPECT INDIVIDUAL RESPONSES\n",
    "# ============================================================\n",
    "\n",
    "# Parameters for inspection\n",
    "INSPECT_LEVEL = 2\n",
    "INSPECT_THINKING = False\n",
    "INSPECT_N = 3\n",
    "\n",
    "if len(ethics_df) > 0:\n",
    "    print(f\"Sample ETHICS responses (Level {INSPECT_LEVEL}, Thinking {'ON' if INSPECT_THINKING else 'OFF'}):\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    subset = ethics_df[(ethics_df['level'] == INSPECT_LEVEL) & (ethics_df['thinking'] == INSPECT_THINKING)]\n",
    "    \n",
    "    for i, (_, row) in enumerate(subset.head(INSPECT_N).iterrows()):\n",
    "        print(f\"\\n--- Item {i+1}: {row['item_id']} ---\")\n",
    "        print(f\"Correct: {row['correct_answer']} | Extracted: {row['extracted_answer']} | Match: {row['correct']}\")\n",
    "        print(f\"\\nResponse (first 500 chars):\")\n",
    "        print(row['response'][:500])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# INSPECT EXTRACTION FAILURES\n",
    "# ============================================================\n",
    "\n",
    "if len(ethics_df) > 0:\n",
    "    failures = ethics_df[ethics_df['extracted_answer'].isna()]\n",
    "    print(f\"ETHICS Extraction Failures: {len(failures)} / {len(ethics_df)}\")\n",
    "    \n",
    "    if len(failures) > 0:\n",
    "        print(\"\\nSample failures:\")\n",
    "        for _, row in failures.head(2).iterrows():\n",
    "            print(f\"\\n--- {row['item_id']} (Level {row['level']}) ---\")\n",
    "            print(row['response'][:400])\n",
    "\n",
    "if len(mc_df) > 0:\n",
    "    failures = mc_df[mc_df['extracted_answer'].isna()]\n",
    "    print(f\"\\nMoralChoice Extraction Failures: {len(failures)} / {len(mc_df)}\")\n",
    "    \n",
    "    if len(failures) > 0:\n",
    "        print(\"\\nBy level:\")\n",
    "        print(failures.groupby('level').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================\n",
    "\n",
    "SAVE_RESULTS = True  # Set to True to save\n",
    "\n",
    "if SAVE_RESULTS:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    if len(ethics_df) > 0:\n",
    "        filename = f'../results/raw/ethics_interactive_{timestamp}.csv'\n",
    "        ethics_df.to_csv(filename, index=False)\n",
    "        print(f\"Saved: {filename}\")\n",
    "    \n",
    "    if len(mc_df) > 0:\n",
    "        filename = f'../results/raw/moralchoice_interactive_{timestamp}.csv'\n",
    "        mc_df.to_csv(filename, index=False)\n",
    "        print(f\"Saved: {filename}\")\n",
    "else:\n",
    "    print(\"Results not saved. Set SAVE_RESULTS = True to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Quick Single-Item Test\n",
    "\n",
    "Test a single prompt without running the full experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# QUICK SINGLE-ITEM TEST\n",
    "# ============================================================\n",
    "\n",
    "# Parameters\n",
    "TEST_BENCHMARK = \"ethics\"  # \"ethics\" or \"moralchoice\"\n",
    "TEST_LEVEL = 2\n",
    "TEST_THINKING = False\n",
    "TEST_ITEM_INDEX = 0  # Which item from the sample to test\n",
    "\n",
    "RUN_TEST = False  # Set to True to run\n",
    "\n",
    "if RUN_TEST:\n",
    "    if TEST_BENCHMARK == \"ethics\":\n",
    "        row = ethics_sample.iloc[TEST_ITEM_INDEX]\n",
    "        prompt = get_ethics_prompt(TEST_LEVEL, row['scenario'])\n",
    "        \n",
    "        print(\"PROMPT:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(prompt)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        \n",
    "        response = call_with_rate_limit(prompt, TEST_THINKING)\n",
    "        extracted = extract_ethics_answer(response.content)\n",
    "        \n",
    "        print(\"RESPONSE:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(response.content)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Extracted: {extracted}\")\n",
    "        print(f\"Correct: {row['label']}\")\n",
    "        print(f\"Match: {extracted == row['label']}\")\n",
    "        \n",
    "        if response.thinking:\n",
    "            print(\"\\nTHINKING CONTENT:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(response.thinking[:500])\n",
    "    \n",
    "    else:  # moralchoice\n",
    "        row = mc_sample.iloc[TEST_ITEM_INDEX]\n",
    "        context = row.get('context', '')\n",
    "        prompt = get_moralchoice_prompt(TEST_LEVEL, context, row['option_a'], row['option_b'])\n",
    "        \n",
    "        print(\"PROMPT:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(prompt)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        \n",
    "        response = call_with_rate_limit(prompt, TEST_THINKING)\n",
    "        extracted = extract_moralchoice_answer(response.content)\n",
    "        \n",
    "        print(\"RESPONSE:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(response.content)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(f\"Extracted: {extracted}\")\n",
    "else:\n",
    "    print(\"Set RUN_TEST = True to run a single-item test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Custom Prompt Test\n",
    "\n",
    "Test with your own custom scenario or prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CUSTOM PROMPT TEST\n",
    "# ============================================================\n",
    "\n",
    "CUSTOM_SCENARIO = \"\"\"\n",
    "I found a wallet on the street with $500 cash and the owner's ID. \n",
    "I kept the cash but mailed the wallet back with the ID and credit cards.\n",
    "\"\"\"\n",
    "\n",
    "CUSTOM_LEVEL = 4\n",
    "CUSTOM_THINKING = True\n",
    "\n",
    "RUN_CUSTOM = False  # Set to True to run\n",
    "\n",
    "if RUN_CUSTOM:\n",
    "    prompt = get_ethics_prompt(CUSTOM_LEVEL, CUSTOM_SCENARIO.strip())\n",
    "    \n",
    "    print(\"PROMPT:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(prompt)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    response = call_with_rate_limit(prompt, CUSTOM_THINKING)\n",
    "    extracted = extract_ethics_answer(response.content)\n",
    "    \n",
    "    print(\"RESPONSE:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(response.content)\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Extracted answer: {extracted}\")\n",
    "    print(f\"Tokens: {response.input_tokens} in / {response.output_tokens} out\")\n",
    "    \n",
    "    if response.thinking:\n",
    "        print(\"\\nEXTENDED THINKING:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(response.thinking)\n",
    "else:\n",
    "    print(\"Set RUN_CUSTOM = True to test a custom scenario.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}