{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Reflection: Comprehensive Analysis\n",
    "\n",
    "**Research Question**: Does prompted reflection improve or impair moral reasoning in LLMs?\n",
    "\n",
    "This notebook analyzes the relationship between reflection levels, extended thinking, and moral judgment accuracy across three benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Color palettes\n",
    "COLORS = {\n",
    "    'no_thinking': '#2ecc71',  # Green\n",
    "    'with_thinking': '#e74c3c',  # Red\n",
    "    'ethics': '#3498db',  # Blue\n",
    "    'moralchoice': '#9b59b6',  # Purple\n",
    "    'morables': '#f39c12',  # Orange\n",
    "}\n",
    "\n",
    "SUBSCALE_COLORS = {\n",
    "    'commonsense': '#1abc9c',\n",
    "    'deontology': '#3498db',\n",
    "    'virtue': '#9b59b6',\n",
    "}\n",
    "\n",
    "AMBIGUITY_COLORS = {\n",
    "    'low': '#27ae60',\n",
    "    'high': '#c0392b',\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path('../outputs/figures').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "results_dir = Path(\"../results/raw\")\n",
    "\n",
    "ethics_df = pd.read_csv(results_dir / \"ethics_checkpoint.csv\") if (results_dir / \"ethics_checkpoint.csv\").exists() else None\n",
    "mc_df = pd.read_csv(results_dir / \"moralchoice_checkpoint.csv\") if (results_dir / \"moralchoice_checkpoint.csv\").exists() else None\n",
    "morables_df = pd.read_csv(results_dir / \"morables_checkpoint.csv\") if (results_dir / \"morables_checkpoint.csv\").exists() else None\n",
    "\n",
    "print(\"Data Loaded:\")\n",
    "print(f\"  ETHICS:      {len(ethics_df) if ethics_df is not None else 0:,} observations\")\n",
    "print(f\"  MoralChoice: {len(mc_df) if mc_df is not None else 0:,} observations\")\n",
    "print(f\"  MORABLES:    {len(morables_df) if morables_df is not None else 0:,} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: The Core Finding — Reflection Decreases Performance\n",
    "\n",
    "This section demonstrates the primary result: higher reflection levels lead to lower accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Accuracy by Reflection Level (Main Result)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (df, name, color) in zip(axes, [(ethics_df, \"ETHICS\", COLORS['ethics']), \n",
    "                                         (morables_df, \"MORABLES\", COLORS['morables'])]):\n",
    "    if df is None or 'correct' not in df.columns:\n",
    "        ax.text(0.5, 0.5, f\"No data for {name}\", ha='center', va='center', transform=ax.transAxes)\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['correct'].notna()]\n",
    "    \n",
    "    # Calculate accuracy by level\n",
    "    acc_by_level = valid.groupby('level')['correct'].agg(['mean', 'std', 'count']).reset_index()\n",
    "    acc_by_level['se'] = acc_by_level['std'] / np.sqrt(acc_by_level['count'])\n",
    "    acc_by_level['mean_pct'] = acc_by_level['mean'] * 100\n",
    "    acc_by_level['se_pct'] = acc_by_level['se'] * 100\n",
    "    \n",
    "    # Plot with error bars\n",
    "    ax.errorbar(acc_by_level['level'], acc_by_level['mean_pct'], \n",
    "                yerr=acc_by_level['se_pct'] * 1.96,  # 95% CI\n",
    "                marker='o', markersize=12, linewidth=3, capsize=5,\n",
    "                color=color, markerfacecolor='white', markeredgewidth=2)\n",
    "    \n",
    "    # Add trend line\n",
    "    z = np.polyfit(acc_by_level['level'], acc_by_level['mean_pct'], 1)\n",
    "    p = np.poly1d(z)\n",
    "    ax.plot(acc_by_level['level'], p(acc_by_level['level']), '--', \n",
    "            color=color, alpha=0.5, linewidth=2, label=f'Trend: {z[0]:+.1f}% per level')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    r, p_val = stats.pearsonr(acc_by_level['level'], acc_by_level['mean_pct'])\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_title(f'{name}: Accuracy Decreases with Reflection\\n(r = {r:.2f}, p = {p_val:.3f})', fontsize=14)\n",
    "    ax.set_xticks(acc_by_level['level'])\n",
    "    ax.set_xticklabels(['L0\\n(Direct)', 'L2\\n(CoT)', 'L4\\n(Devil\\'s\\nAdvocate)', 'L5\\n(Two-Pass)'])\n",
    "    ax.set_ylim(50, 100)\n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    # Add annotation for drop\n",
    "    l0_acc = acc_by_level[acc_by_level['level'] == 0]['mean_pct'].values[0]\n",
    "    l5_acc = acc_by_level[acc_by_level['level'] == acc_by_level['level'].max()]['mean_pct'].values[0]\n",
    "    ax.annotate(f'Δ = {l5_acc - l0_acc:+.1f}%', \n",
    "                xy=(0.95, 0.05), xycoords='axes fraction',\n",
    "                fontsize=14, fontweight='bold', color='red',\n",
    "                ha='right', va='bottom',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', edgecolor='red', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Primary Finding: More Reflection → Lower Accuracy', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/fig1_reflection_decreases_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Thinking × Level Interaction\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(ethics_df, \"ETHICS\"), (morables_df, \"MORABLES\")]):\n",
    "    if df is None or 'correct' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['correct'].notna()]\n",
    "    \n",
    "    for thinking, color, label in [(False, COLORS['no_thinking'], 'No Extended Thinking'),\n",
    "                                    (True, COLORS['with_thinking'], 'With Extended Thinking')]:\n",
    "        subset = valid[valid['thinking'] == thinking]\n",
    "        acc = subset.groupby('level')['correct'].mean() * 100\n",
    "        \n",
    "        ax.plot(acc.index, acc.values, marker='o', markersize=10, \n",
    "                linewidth=2.5, color=color, label=label)\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'{name}: Extended Thinking Effect by Level')\n",
    "    ax.set_xticks(valid['level'].unique())\n",
    "    ax.legend(loc='lower left')\n",
    "    ax.set_ylim(50, 100)\n",
    "    \n",
    "    # Highlight L0 difference\n",
    "    l0_no = valid[(valid['level'] == 0) & (valid['thinking'] == False)]['correct'].mean() * 100\n",
    "    l0_yes = valid[(valid['level'] == 0) & (valid['thinking'] == True)]['correct'].mean() * 100\n",
    "    \n",
    "    ax.annotate(f'L0: Thinking hurts\\nby {l0_no - l0_yes:.1f}%',\n",
    "                xy=(0, l0_no), xytext=(0.5, l0_no + 5),\n",
    "                arrowprops=dict(arrowstyle='->', color='gray'),\n",
    "                fontsize=10, ha='center')\n",
    "\n",
    "plt.suptitle('Extended Thinking Interaction with Reflection Level', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/fig2_thinking_interaction.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Combined Heatmap - All Conditions\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(ethics_df, \"ETHICS\"), (mc_df, \"MoralChoice\"), (morables_df, \"MORABLES\")]):\n",
    "    if df is None:\n",
    "        ax.text(0.5, 0.5, f\"No data\", ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(name)\n",
    "        continue\n",
    "    \n",
    "    if 'correct' in df.columns:\n",
    "        valid = df[df['correct'].notna()]\n",
    "        metric = valid.groupby(['level', 'thinking'])['correct'].mean() * 100\n",
    "        metric_name = 'Accuracy (%)'\n",
    "    else:\n",
    "        # For MoralChoice, use extraction rate or confidence\n",
    "        valid = df[df['extracted_answer'].notna()]\n",
    "        if 'confidence' in df.columns:\n",
    "            metric = valid.groupby(['level', 'thinking'])['confidence'].mean()\n",
    "            metric_name = 'Confidence'\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    pivot = metric.unstack()\n",
    "    pivot.columns = ['No Thinking', 'With Thinking']\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "                center=pivot.values.mean(), ax=ax,\n",
    "                cbar_kws={'label': metric_name})\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_xlabel('Thinking Mode')\n",
    "    ax.set_ylabel('Reflection Level')\n",
    "\n",
    "plt.suptitle('Performance Across All Conditions', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/fig3_heatmap_all_conditions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Subcategory Analysis\n",
    "\n",
    "Analyzing how the reflection effect varies across moral frameworks (ETHICS subscales) and ambiguity levels (MoralChoice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: ETHICS Accuracy by Subscale\n",
    "\n",
    "if ethics_df is not None and 'subscale' in ethics_df.columns:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    valid = ethics_df[ethics_df['correct'].notna()]\n",
    "    \n",
    "    for ax, subscale in zip(axes, ['commonsense', 'deontology', 'virtue']):\n",
    "        subset = valid[valid['subscale'] == subscale]\n",
    "        \n",
    "        for thinking, color, label in [(False, COLORS['no_thinking'], 'No Thinking'),\n",
    "                                        (True, COLORS['with_thinking'], 'With Thinking')]:\n",
    "            sub_think = subset[subset['thinking'] == thinking]\n",
    "            acc = sub_think.groupby('level')['correct'].mean() * 100\n",
    "            ax.plot(acc.index, acc.values, marker='o', markersize=8, \n",
    "                    linewidth=2, color=color, label=label)\n",
    "        \n",
    "        # Overall accuracy for this subscale\n",
    "        overall = subset['correct'].mean() * 100\n",
    "        \n",
    "        ax.set_xlabel('Reflection Level')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_title(f'{subscale.capitalize()}\\n(Overall: {overall:.1f}%)')\n",
    "        ax.set_xticks(valid['level'].unique())\n",
    "        ax.legend(loc='lower left', fontsize=9)\n",
    "        ax.set_ylim(40, 100)\n",
    "        ax.axhline(y=50, color='gray', linestyle='--', alpha=0.3, label='Chance')\n",
    "    \n",
    "    plt.suptitle('ETHICS: Reflection Effect by Moral Framework', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/fig4_ethics_by_subscale.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ETHICS data with subscale not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: ETHICS Subscale Comparison (Bar Chart)\n",
    "\n",
    "if ethics_df is not None and 'subscale' in ethics_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    valid = ethics_df[ethics_df['correct'].notna()]\n",
    "    \n",
    "    # Aggregate by subscale and level\n",
    "    agg = valid.groupby(['subscale', 'level'])['correct'].mean().reset_index()\n",
    "    agg['correct'] = agg['correct'] * 100\n",
    "    \n",
    "    # Create grouped bar chart\n",
    "    subscales = ['commonsense', 'deontology', 'virtue']\n",
    "    levels = sorted(agg['level'].unique())\n",
    "    x = np.arange(len(subscales))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, level in enumerate(levels):\n",
    "        level_data = agg[agg['level'] == level]\n",
    "        values = [level_data[level_data['subscale'] == s]['correct'].values[0] for s in subscales]\n",
    "        bars = ax.bar(x + i*width, values, width, label=f'Level {level}')\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, val in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{val:.0f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Moral Framework')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title('ETHICS: Accuracy by Subscale and Reflection Level')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels([s.capitalize() for s in subscales])\n",
    "    ax.legend(title='Reflection Level')\n",
    "    ax.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/fig5_ethics_subscale_bars.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 6: MoralChoice by Ambiguity\n",
    "\n",
    "if mc_df is not None and 'ambiguity' in mc_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    valid = mc_df[mc_df['extracted_answer'].notna()]\n",
    "    \n",
    "    # Left: Choice distribution by ambiguity and level\n",
    "    ax = axes[0]\n",
    "    \n",
    "    for amb, color in AMBIGUITY_COLORS.items():\n",
    "        subset = valid[valid['ambiguity'] == amb]\n",
    "        pct_a = subset.groupby('level').apply(lambda x: (x['extracted_answer'] == 'A').mean() * 100)\n",
    "        ax.plot(pct_a.index, pct_a.values, marker='o', markersize=10,\n",
    "                linewidth=2.5, color=color, label=f'{amb.capitalize()} Ambiguity')\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level')\n",
    "    ax.set_ylabel('% Choosing Option A')\n",
    "    ax.set_title('MoralChoice: Option Preference by Ambiguity')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(50, 100)\n",
    "    ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Right: Confidence by ambiguity\n",
    "    ax = axes[1]\n",
    "    \n",
    "    if 'confidence' in valid.columns:\n",
    "        conf_valid = valid[valid['confidence'].notna()]\n",
    "        \n",
    "        for amb, color in AMBIGUITY_COLORS.items():\n",
    "            subset = conf_valid[conf_valid['ambiguity'] == amb]\n",
    "            conf = subset.groupby('level')['confidence'].mean()\n",
    "            ax.plot(conf.index, conf.values, marker='s', markersize=10,\n",
    "                    linewidth=2.5, color=color, label=f'{amb.capitalize()} Ambiguity')\n",
    "        \n",
    "        ax.set_xlabel('Reflection Level')\n",
    "        ax.set_ylabel('Mean Confidence')\n",
    "        ax.set_title('MoralChoice: Confidence by Ambiguity')\n",
    "        ax.legend()\n",
    "        ax.set_ylim(50, 100)\n",
    "    \n",
    "    plt.suptitle('MoralChoice: Ambiguity Effects', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/fig6_moralchoice_ambiguity.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 7: Confidence Calibration Curves\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (df, name, color) in zip(axes, [(ethics_df, \"ETHICS\", COLORS['ethics']), \n",
    "                                         (morables_df, \"MORABLES\", COLORS['morables'])]):\n",
    "    if df is None or 'confidence' not in df.columns or 'correct' not in df.columns:\n",
    "        ax.text(0.5, 0.5, f\"No data for {name}\", ha='center', va='center', transform=ax.transAxes)\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['confidence'].notna() & df['correct'].notna()].copy()\n",
    "    \n",
    "    # Bin confidence\n",
    "    bins = [0, 20, 40, 60, 80, 100]\n",
    "    labels = ['0-20', '21-40', '41-60', '61-80', '81-100']\n",
    "    valid['conf_bin'] = pd.cut(valid['confidence'], bins=bins, labels=labels)\n",
    "    \n",
    "    calib = valid.groupby('conf_bin', observed=True).agg(\n",
    "        accuracy=('correct', 'mean'),\n",
    "        count=('correct', 'count')\n",
    "    ).reset_index()\n",
    "    calib['accuracy_pct'] = calib['accuracy'] * 100\n",
    "    \n",
    "    # Expected (perfect calibration)\n",
    "    midpoints = {'0-20': 10, '21-40': 30, '41-60': 50, '61-80': 70, '81-100': 90}\n",
    "    calib['expected'] = calib['conf_bin'].map(midpoints)\n",
    "    \n",
    "    x = range(len(calib))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar([i - width/2 for i in x], calib['accuracy_pct'], width, \n",
    "           label='Actual Accuracy', color=color, alpha=0.7)\n",
    "    ax.bar([i + width/2 for i in x], calib['expected'], width,\n",
    "           label='Perfect Calibration', color='gray', alpha=0.5)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(calib['conf_bin'])\n",
    "    ax.set_xlabel('Confidence Range')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'{name}: Confidence Calibration')\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 105)\n",
    "    \n",
    "    # Add count annotations\n",
    "    for i, (_, row) in enumerate(calib.iterrows()):\n",
    "        ax.text(i - width/2, row['accuracy_pct'] + 2, f'n={row[\"count\"]}', \n",
    "                ha='center', fontsize=8, color='gray')\n",
    "\n",
    "plt.suptitle('Confidence Calibration: Model is Overconfident', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/fig7_confidence_calibration.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8: Confidence by Subscale (ETHICS)\n",
    "\n",
    "if ethics_df is not None and 'subscale' in ethics_df.columns and 'confidence' in ethics_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    valid = ethics_df[ethics_df['confidence'].notna() & ethics_df['correct'].notna()]\n",
    "    \n",
    "    # Left: Mean confidence by subscale\n",
    "    ax = axes[0]\n",
    "    conf_by_sub = valid.groupby('subscale')['confidence'].mean()\n",
    "    acc_by_sub = valid.groupby('subscale')['correct'].mean() * 100\n",
    "    \n",
    "    x = np.arange(len(conf_by_sub))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, conf_by_sub.values, width, label='Confidence', color='steelblue')\n",
    "    bars2 = ax.bar(x + width/2, acc_by_sub.values, width, label='Accuracy', color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Subscale')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Confidence vs Accuracy by Subscale')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([s.capitalize() for s in conf_by_sub.index])\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add gap annotations\n",
    "    for i, (sub, conf) in enumerate(conf_by_sub.items()):\n",
    "        acc = acc_by_sub[sub]\n",
    "        gap = conf - acc\n",
    "        ax.annotate(f'+{gap:.0f}', xy=(i, max(conf, acc) + 2), \n",
    "                    ha='center', fontsize=10, color='red' if gap > 5 else 'gray')\n",
    "    \n",
    "    # Right: Confidence when correct vs incorrect\n",
    "    ax = axes[1]\n",
    "    \n",
    "    conf_correct = valid.groupby(['subscale', 'correct'])['confidence'].mean().unstack()\n",
    "    conf_correct.columns = ['Incorrect', 'Correct']\n",
    "    \n",
    "    conf_correct.plot(kind='bar', ax=ax, color=['#e74c3c', '#2ecc71'])\n",
    "    ax.set_xlabel('Subscale')\n",
    "    ax.set_ylabel('Mean Confidence')\n",
    "    ax.set_title('Confidence When Correct vs Incorrect')\n",
    "    ax.set_xticklabels([s.capitalize() for s in conf_correct.index], rotation=0)\n",
    "    ax.legend(title='Answer')\n",
    "    \n",
    "    plt.suptitle('ETHICS: Confidence Analysis by Subscale', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/fig8_confidence_by_subscale.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 9: Confidence by Reflection Level\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(ethics_df, \"ETHICS\"), (morables_df, \"MORABLES\")]):\n",
    "    if df is None or 'confidence' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['confidence'].notna()]\n",
    "    \n",
    "    # Violin plot of confidence by level\n",
    "    levels = sorted(valid['level'].unique())\n",
    "    data = [valid[valid['level'] == l]['confidence'].values for l in levels]\n",
    "    \n",
    "    parts = ax.violinplot(data, positions=levels, showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Color the violins\n",
    "    for pc in parts['bodies']:\n",
    "        pc.set_facecolor('steelblue')\n",
    "        pc.set_alpha(0.6)\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level')\n",
    "    ax.set_ylabel('Confidence')\n",
    "    ax.set_title(f'{name}: Confidence Distribution by Level')\n",
    "    ax.set_xticks(levels)\n",
    "    ax.set_ylim(0, 105)\n",
    "\n",
    "plt.suptitle('Confidence Distributions Across Reflection Levels', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/fig9_confidence_by_level.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Summary by Benchmark and Condition\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for df, name in [(ethics_df, \"ETHICS\"), (mc_df, \"MoralChoice\"), (morables_df, \"MORABLES\")]:\n",
    "    if df is None:\n",
    "        continue\n",
    "    \n",
    "    for level in sorted(df['level'].unique()):\n",
    "        for thinking in [False, True]:\n",
    "            subset = df[(df['level'] == level) & (df['thinking'] == thinking)]\n",
    "            \n",
    "            row = {\n",
    "                'Benchmark': name,\n",
    "                'Level': level,\n",
    "                'Thinking': 'On' if thinking else 'Off',\n",
    "                'N': len(subset),\n",
    "            }\n",
    "            \n",
    "            if 'extracted_answer' in subset.columns:\n",
    "                row['Extraction %'] = round(subset['extracted_answer'].notna().mean() * 100, 1)\n",
    "            \n",
    "            if 'correct' in subset.columns:\n",
    "                valid = subset[subset['correct'].notna()]\n",
    "                if len(valid) > 0:\n",
    "                    row['Accuracy %'] = round(valid['correct'].mean() * 100, 1)\n",
    "            \n",
    "            if 'confidence' in subset.columns:\n",
    "                conf = subset['confidence'].dropna()\n",
    "                if len(conf) > 0:\n",
    "                    row['Mean Conf'] = round(conf.mean(), 1)\n",
    "                    row['Conf Std'] = round(conf.std(), 1)\n",
    "            \n",
    "            summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Summary Statistics:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Save\n",
    "summary_df.to_csv('../outputs/summary_by_condition.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: ETHICS by Subscale\n",
    "\n",
    "if ethics_df is not None and 'subscale' in ethics_df.columns:\n",
    "    subscale_summary = []\n",
    "    \n",
    "    valid = ethics_df[ethics_df['correct'].notna()]\n",
    "    \n",
    "    for subscale in valid['subscale'].unique():\n",
    "        for level in sorted(valid['level'].unique()):\n",
    "            subset = valid[(valid['subscale'] == subscale) & (valid['level'] == level)]\n",
    "            \n",
    "            row = {\n",
    "                'Subscale': subscale.capitalize(),\n",
    "                'Level': level,\n",
    "                'N': len(subset),\n",
    "                'Accuracy %': round(subset['correct'].mean() * 100, 1),\n",
    "            }\n",
    "            \n",
    "            if 'confidence' in subset.columns:\n",
    "                conf = subset['confidence'].dropna()\n",
    "                if len(conf) > 0:\n",
    "                    row['Mean Conf'] = round(conf.mean(), 1)\n",
    "            \n",
    "            subscale_summary.append(row)\n",
    "    \n",
    "    subscale_df = pd.DataFrame(subscale_summary)\n",
    "    \n",
    "    print(\"\\nETHICS by Subscale:\")\n",
    "    pivot = subscale_df.pivot(index='Subscale', columns='Level', values='Accuracy %')\n",
    "    display(pivot)\n",
    "    \n",
    "    subscale_df.to_csv('../outputs/ethics_by_subscale.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3: Key Findings Summary\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KEY FINDINGS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "findings = []\n",
    "\n",
    "for df, name in [(ethics_df, \"ETHICS\"), (morables_df, \"MORABLES\")]:\n",
    "    if df is None or 'correct' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['correct'].notna()]\n",
    "    \n",
    "    # Accuracy at L0 vs L5\n",
    "    l0 = valid[valid['level'] == 0]['correct'].mean() * 100\n",
    "    l5 = valid[valid['level'] == valid['level'].max()]['correct'].mean() * 100\n",
    "    \n",
    "    # Best and worst conditions\n",
    "    acc_by_cond = valid.groupby(['level', 'thinking'])['correct'].mean() * 100\n",
    "    best = acc_by_cond.idxmax()\n",
    "    worst = acc_by_cond.idxmin()\n",
    "    \n",
    "    # Thinking effect at L0\n",
    "    l0_no_think = valid[(valid['level'] == 0) & (valid['thinking'] == False)]['correct'].mean() * 100\n",
    "    l0_with_think = valid[(valid['level'] == 0) & (valid['thinking'] == True)]['correct'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Reflection effect: L0 ({l0:.1f}%) → Max Level ({l5:.1f}%) = {l5-l0:+.1f}%\")\n",
    "    print(f\"  Best condition: Level {best[0]}, Thinking {'ON' if best[1] else 'OFF'} ({acc_by_cond[best]:.1f}%)\")\n",
    "    print(f\"  Worst condition: Level {worst[0]}, Thinking {'ON' if worst[1] else 'OFF'} ({acc_by_cond[worst]:.1f}%)\")\n",
    "    print(f\"  L0 Thinking effect: {l0_with_think - l0_no_think:+.1f}%\")\n",
    "    \n",
    "    if 'confidence' in valid.columns:\n",
    "        mean_conf = valid['confidence'].mean()\n",
    "        mean_acc = valid['correct'].mean() * 100\n",
    "        print(f\"  Calibration gap: Confidence ({mean_conf:.1f}) - Accuracy ({mean_acc:.1f}) = {mean_conf - mean_acc:+.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for key findings\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"STATISTICAL TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for df, name in [(ethics_df, \"ETHICS\"), (morables_df, \"MORABLES\")]:\n",
    "    if df is None or 'correct' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['correct'].notna()]\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # 1. Correlation between level and accuracy\n",
    "    acc_by_level = valid.groupby('level')['correct'].mean()\n",
    "    r, p = stats.pearsonr(acc_by_level.index, acc_by_level.values)\n",
    "    print(f\"  Level-Accuracy correlation: r={r:.3f}, p={p:.4f}\")\n",
    "    \n",
    "    # 2. L0 vs L5 comparison\n",
    "    l0_correct = valid[valid['level'] == 0]['correct']\n",
    "    l5_correct = valid[valid['level'] == valid['level'].max()]['correct']\n",
    "    \n",
    "    # Chi-square test\n",
    "    contingency = pd.crosstab(valid['level'].isin([0]), valid['correct'])\n",
    "    if len(contingency) > 1:\n",
    "        chi2, p, dof, expected = stats.chi2_contingency(contingency)\n",
    "        print(f\"  L0 vs Others (chi-square): χ²={chi2:.2f}, p={p:.4f}\")\n",
    "    \n",
    "    # 3. Thinking effect at L0\n",
    "    l0_no_think = valid[(valid['level'] == 0) & (valid['thinking'] == False)]['correct']\n",
    "    l0_with_think = valid[(valid['level'] == 0) & (valid['thinking'] == True)]['correct']\n",
    "    \n",
    "    if len(l0_no_think) > 0 and len(l0_with_think) > 0:\n",
    "        # Proportion z-test\n",
    "        p1 = l0_no_think.mean()\n",
    "        p2 = l0_with_think.mean()\n",
    "        n1, n2 = len(l0_no_think), len(l0_with_think)\n",
    "        p_pooled = (p1*n1 + p2*n2) / (n1 + n2)\n",
    "        se = np.sqrt(p_pooled * (1-p_pooled) * (1/n1 + 1/n2))\n",
    "        z = (p1 - p2) / se if se > 0 else 0\n",
    "        p_val = 2 * (1 - stats.norm.cdf(abs(z)))\n",
    "        print(f\"  L0 Thinking effect (z-test): z={z:.2f}, p={p_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all figures list\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIGURES GENERATED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "figures = [\n",
    "    (\"fig1_reflection_decreases_accuracy.png\", \"Primary finding: accuracy decreases with reflection level\"),\n",
    "    (\"fig2_thinking_interaction.png\", \"Extended thinking interaction with reflection level\"),\n",
    "    (\"fig3_heatmap_all_conditions.png\", \"Heatmap of performance across all conditions\"),\n",
    "    (\"fig4_ethics_by_subscale.png\", \"ETHICS accuracy by moral framework\"),\n",
    "    (\"fig5_ethics_subscale_bars.png\", \"ETHICS subscale comparison bar chart\"),\n",
    "    (\"fig6_moralchoice_ambiguity.png\", \"MoralChoice analysis by ambiguity level\"),\n",
    "    (\"fig7_confidence_calibration.png\", \"Confidence calibration curves\"),\n",
    "    (\"fig8_confidence_by_subscale.png\", \"ETHICS confidence analysis by subscale\"),\n",
    "    (\"fig9_confidence_by_level.png\", \"Confidence distributions by reflection level\"),\n",
    "]\n",
    "\n",
    "for fname, desc in figures:\n",
    "    path = Path(f'../outputs/figures/{fname}')\n",
    "    status = \"✓\" if path.exists() else \"✗\"\n",
    "    print(f\"  {status} {fname}: {desc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
