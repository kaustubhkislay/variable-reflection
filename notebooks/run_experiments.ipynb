{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Reflection Experiment Runner\n",
    "\n",
    "Interactive notebook for running moral reasoning experiments with configurable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, '..')\n",
    "os.chdir('..')\n",
    "\n",
    "# Load environment variables\n",
    "with open('.env', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line and not line.startswith('#'):\n",
    "            match = re.match(r'(\\w+)\\s*=\\s*[\"\\']?([^\"\\']+)[\"\\']?', line)\n",
    "            if match:\n",
    "                os.environ[match.group(1)] = match.group(2)\n",
    "\n",
    "import config\n",
    "print(f\"Model: {config.MODEL}\")\n",
    "print(f\"Rate limit: {config.CALLS_PER_MINUTE}/min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Adjust these parameters as needed before running experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT PARAMETERS - MODIFY THESE\n",
    "# =============================================================================\n",
    "\n",
    "# Reflection levels to test (0-5)\n",
    "# 0=Direct, 1=Minimal, 2=CoT, 3=Structured, 4=Adversarial, 5=Two-Pass\n",
    "LEVELS = [0]\n",
    "\n",
    "# Thinking conditions\n",
    "# False = Thinking OFF (temperature=0, max_tokens=1000)\n",
    "# True = Thinking ON (extended thinking with 2000 token budget)\n",
    "THINKING_CONDITIONS = [False, True]\n",
    "\n",
    "# Number of runs per condition (for measuring consistency)\n",
    "N_RUNS = 1\n",
    "\n",
    "# Sample size per benchmark (None = full dataset)\n",
    "SAMPLE_SIZE = 6  # Start small for testing\n",
    "\n",
    "# Include confidence scoring in prompts\n",
    "INCLUDE_CONFIDENCE = True\n",
    "\n",
    "# Which benchmarks to run\n",
    "RUN_ETHICS = True\n",
    "RUN_MORALCHOICE = True\n",
    "RUN_MORABLES = True\n",
    "\n",
    "# Execution mode\n",
    "USE_ASYNC = True  # True = parallel datasets (~2.5x faster), False = sequential\n",
    "\n",
    "# =============================================================================\n",
    "# Summary\n",
    "# =============================================================================\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Levels: {LEVELS}\")\n",
    "print(f\"Thinking conditions: {THINKING_CONDITIONS}\")\n",
    "print(f\"Runs per condition: {N_RUNS}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE or 'Full dataset'}\")\n",
    "print(f\"Confidence scoring: {INCLUDE_CONFIDENCE}\")\n",
    "print(f\"Benchmarks: \", end=\"\")\n",
    "benchmarks = []\n",
    "if RUN_ETHICS: benchmarks.append(\"ETHICS\")\n",
    "if RUN_MORALCHOICE: benchmarks.append(\"MoralChoice\")\n",
    "if RUN_MORABLES: benchmarks.append(\"MORABLES\")\n",
    "print(\", \".join(benchmarks))\n",
    "print(f\"Mode: {'Async (parallel)' if USE_ASYNC else 'Sync (sequential)'}\")\n",
    "\n",
    "# Estimate API calls\n",
    "calls_per_item = len(LEVELS) - 1 + 2  # Level 5 needs 2 calls\n",
    "calls_per_item *= len(THINKING_CONDITIONS) * N_RUNS\n",
    "estimated_items = (SAMPLE_SIZE or 100) * len(benchmarks)\n",
    "estimated_calls = estimated_items * calls_per_item\n",
    "print(f\"\\nEstimated API calls: ~{estimated_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from run_experiment module\n",
    "import run_experiment as exp\n",
    "\n",
    "# Override module-level settings with our config\n",
    "exp.LEVELS = LEVELS\n",
    "exp.THINKING_CONDITIONS = THINKING_CONDITIONS\n",
    "exp.N_RUNS = N_RUNS\n",
    "\n",
    "# Import API utilities for async mode\n",
    "from src.api import reset_rate_limiter, get_rate_stats\n",
    "\n",
    "print(\"Experiment module loaded and configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data files exist\n",
    "data_files = {\n",
    "    'ETHICS': 'data/ethics_sample.csv',\n",
    "    'MoralChoice': 'data/moralchoice_sample.csv',\n",
    "    'MORABLES': 'data/morables/morables_sample.csv'\n",
    "}\n",
    "\n",
    "print(\"Data file status:\")\n",
    "for name, path in data_files.items():\n",
    "    if Path(path).exists():\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"  {name}: {path} ({len(df)} items)\")\n",
    "    else:\n",
    "        print(f\"  {name}: {path} (NOT FOUND - run prepare_data.py)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "Choose one of the methods below based on your configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A: Run All Selected Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "Path(\"results/raw\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"results/processed\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start_time = datetime.now()\n",
    "results = {}\n",
    "\n",
    "if USE_ASYNC:\n",
    "    # Async mode - run benchmarks in parallel\n",
    "    print(\"Running in ASYNC mode (parallel datasets)...\")\n",
    "    reset_rate_limiter()\n",
    "    \n",
    "    async def run_all_async():\n",
    "        results_queue = asyncio.Queue()\n",
    "        \n",
    "        # Start checkpoint writer\n",
    "        checkpoint_task = asyncio.create_task(exp.checkpoint_writer(results_queue))\n",
    "        \n",
    "        tasks = []\n",
    "        task_names = []\n",
    "        \n",
    "        if RUN_ETHICS:\n",
    "            tasks.append(exp.run_ethics_experiment_async(results_queue, SAMPLE_SIZE, INCLUDE_CONFIDENCE))\n",
    "            task_names.append('ethics')\n",
    "        if RUN_MORALCHOICE:\n",
    "            tasks.append(exp.run_moralchoice_experiment_async(results_queue, SAMPLE_SIZE, INCLUDE_CONFIDENCE))\n",
    "            task_names.append('moralchoice')\n",
    "        if RUN_MORABLES:\n",
    "            tasks.append(exp.run_morables_experiment_async(results_queue, SAMPLE_SIZE, INCLUDE_CONFIDENCE))\n",
    "            task_names.append('morables')\n",
    "        \n",
    "        all_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        \n",
    "        # Stop checkpoint writer\n",
    "        checkpoint_task.cancel()\n",
    "        try:\n",
    "            await checkpoint_task\n",
    "        except asyncio.CancelledError:\n",
    "            pass\n",
    "        \n",
    "        return dict(zip(task_names, all_results))\n",
    "    \n",
    "    results = await run_all_async()\n",
    "    \n",
    "else:\n",
    "    # Sync mode - run benchmarks sequentially\n",
    "    print(\"Running in SYNC mode (sequential)...\")\n",
    "    \n",
    "    if RUN_ETHICS:\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\nRUNNING ETHICS\\n\" + \"=\"*50)\n",
    "        results['ethics'] = exp.run_ethics_experiment(SAMPLE_SIZE, INCLUDE_CONFIDENCE)\n",
    "    \n",
    "    if RUN_MORALCHOICE:\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\nRUNNING MORALCHOICE\\n\" + \"=\"*50)\n",
    "        results['moralchoice'] = exp.run_moralchoice_experiment(SAMPLE_SIZE, INCLUDE_CONFIDENCE)\n",
    "    \n",
    "    if RUN_MORABLES:\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\nRUNNING MORABLES\\n\" + \"=\"*50)\n",
    "        results['morables'] = exp.run_morables_experiment(SAMPLE_SIZE, INCLUDE_CONFIDENCE)\n",
    "\n",
    "end_time = datetime.now()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Duration: {duration}\")\n",
    "\n",
    "if USE_ASYNC:\n",
    "    stats = get_rate_stats()\n",
    "    print(f\"Total API calls: {stats['calls']}\")\n",
    "    print(f\"Effective rate: {stats['rate']} calls/min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Run Individual Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ETHICS only\n",
    "ethics_results = exp.run_ethics_experiment(SAMPLE_SIZE, INCLUDE_CONFIDENCE)\n",
    "print(f\"ETHICS: {len(ethics_results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MoralChoice only\n",
    "mc_results = exp.run_moralchoice_experiment(SAMPLE_SIZE, INCLUDE_CONFIDENCE)\n",
    "print(f\"MoralChoice: {len(mc_results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MORABLES only\n",
    "morables_results = exp.run_morables_experiment(SAMPLE_SIZE, INCLUDE_CONFIDENCE)\n",
    "print(f\"MORABLES: {len(morables_results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "for name, data in results.items():\n",
    "    if isinstance(data, Exception):\n",
    "        print(f\"{name}: FAILED - {data}\")\n",
    "        continue\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        # Save to processed folder\n",
    "        output_path = f\"results/processed/{name}_results.csv\"\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"{name}: Saved {len(df)} results to {output_path}\")\n",
    "        \n",
    "        # Also save timestamped version\n",
    "        timestamped_path = f\"results/processed/{name}_results_{timestamp}.csv\"\n",
    "        df.to_csv(timestamped_path, index=False)\n",
    "        print(f\"  Also saved to: {timestamped_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_results(name, data):\n",
    "    \"\"\"Print summary statistics for a benchmark.\"\"\"\n",
    "    if isinstance(data, Exception):\n",
    "        print(f\"{name}: FAILED\")\n",
    "        return\n",
    "    \n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        print(f\"{name}: No results\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name.upper()} SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total observations: {len(df)}\")\n",
    "    \n",
    "    # Extraction success rate\n",
    "    if 'extracted_answer' in df.columns:\n",
    "        extraction_rate = df['extracted_answer'].notna().mean()\n",
    "        print(f\"Extraction success rate: {extraction_rate:.1%}\")\n",
    "    \n",
    "    # Accuracy (for ETHICS and MORABLES)\n",
    "    if 'correct' in df.columns:\n",
    "        valid = df[df['correct'].notna()]\n",
    "        if len(valid) > 0:\n",
    "            print(f\"Overall accuracy: {valid['correct'].mean():.1%}\")\n",
    "            \n",
    "            # By level\n",
    "            print(\"\\nAccuracy by level:\")\n",
    "            for level in sorted(df['level'].unique()):\n",
    "                level_df = valid[valid['level'] == level]\n",
    "                if len(level_df) > 0:\n",
    "                    print(f\"  Level {level}: {level_df['correct'].mean():.1%}\")\n",
    "            \n",
    "            # By thinking condition\n",
    "            print(\"\\nAccuracy by thinking:\")\n",
    "            for thinking in [False, True]:\n",
    "                think_df = valid[valid['thinking'] == thinking]\n",
    "                if len(think_df) > 0:\n",
    "                    label = \"ON\" if thinking else \"OFF\"\n",
    "                    print(f\"  Thinking {label}: {think_df['correct'].mean():.1%}\")\n",
    "    \n",
    "    # Confidence stats\n",
    "    if 'confidence' in df.columns:\n",
    "        valid_conf = df[df['confidence'].notna()]\n",
    "        if len(valid_conf) > 0:\n",
    "            print(f\"\\nConfidence stats:\")\n",
    "            print(f\"  Mean: {valid_conf['confidence'].mean():.1f}\")\n",
    "            print(f\"  Std: {valid_conf['confidence'].std():.1f}\")\n",
    "            print(f\"  Range: {valid_conf['confidence'].min():.0f} - {valid_conf['confidence'].max():.0f}\")\n",
    "\n",
    "# Summarize all results\n",
    "for name, data in results.items():\n",
    "    summarize_results(name, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis by Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pivot_table(name, data):\n",
    "    \"\"\"Create pivot table of accuracy by level and thinking condition.\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    if df is None or 'correct' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    valid = df[df['correct'].notna()].copy()\n",
    "    if len(valid) == 0:\n",
    "        return None\n",
    "    \n",
    "    valid['thinking_label'] = valid['thinking'].map({True: 'Thinking ON', False: 'Thinking OFF'})\n",
    "    \n",
    "    pivot = valid.pivot_table(\n",
    "        values='correct',\n",
    "        index='level',\n",
    "        columns='thinking_label',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    return pivot\n",
    "\n",
    "# Create pivot tables for each benchmark\n",
    "for name, data in results.items():\n",
    "    if isinstance(data, Exception) or data is None:\n",
    "        continue\n",
    "    \n",
    "    pivot = create_pivot_table(name, data)\n",
    "    if pivot is not None:\n",
    "        print(f\"\\n{name.upper()} - Accuracy by Level and Thinking\")\n",
    "        display(pivot.style.format(\"{:.1%}\").background_gradient(cmap='RdYlGn', vmin=0, vmax=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Sample Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_responses(name, data, n=3):\n",
    "    \"\"\"Show sample responses from results.\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = data\n",
    "    \n",
    "    if df is None or len(df) == 0:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name.upper()} - Sample Responses\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    sample = df.sample(min(n, len(df)))\n",
    "    \n",
    "    for i, (_, row) in enumerate(sample.iterrows()):\n",
    "        print(f\"\\n--- Sample {i+1} ---\")\n",
    "        print(f\"Level: {row['level']}, Thinking: {'ON' if row['thinking'] else 'OFF'}\")\n",
    "        \n",
    "        if 'extracted_answer' in row:\n",
    "            print(f\"Extracted answer: {row['extracted_answer']}\")\n",
    "        \n",
    "        if 'correct' in row:\n",
    "            print(f\"Correct: {row['correct']}\")\n",
    "        \n",
    "        if 'confidence' in row and pd.notna(row['confidence']):\n",
    "            print(f\"Confidence: {row['confidence']}\")\n",
    "        \n",
    "        if 'response' in row:\n",
    "            response = str(row['response'])[:500]\n",
    "            print(f\"\\nResponse preview:\\n{response}...\")\n",
    "\n",
    "# Show samples from each benchmark\n",
    "for name, data in results.items():\n",
    "    if not isinstance(data, Exception) and data is not None:\n",
    "        show_sample_responses(name, data, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Previous Results\n",
    "\n",
    "Use this to load and analyze previously saved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results from files\n",
    "def load_results():\n",
    "    \"\"\"Load all available results files.\"\"\"\n",
    "    loaded = {}\n",
    "    \n",
    "    files = {\n",
    "        'ethics': 'results/processed/ethics_results.csv',\n",
    "        'moralchoice': 'results/processed/moralchoice_results.csv',\n",
    "        'morables': 'results/processed/morables_results.csv'\n",
    "    }\n",
    "    \n",
    "    for name, path in files.items():\n",
    "        if Path(path).exists():\n",
    "            loaded[name] = pd.read_csv(path)\n",
    "            print(f\"Loaded {name}: {len(loaded[name])} results\")\n",
    "        else:\n",
    "            print(f\"{name}: No results file found\")\n",
    "    \n",
    "    return loaded\n",
    "\n",
    "# Uncomment to load previous results\n",
    "# results = load_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Experiment Run\n",
    "\n",
    "Use this section to run experiments with specific custom parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run just Level 0 and Level 5 to compare direct vs reflective\n",
    "\n",
    "# Temporarily override settings\n",
    "exp.LEVELS = [0, 5]  # Only direct and two-pass\n",
    "exp.THINKING_CONDITIONS = [False]  # Only thinking OFF\n",
    "exp.N_RUNS = 1\n",
    "\n",
    "# Run a quick comparison\n",
    "# comparison_results = exp.run_ethics_experiment(sample_size=20, include_confidence=True)\n",
    "# print(f\"Quick comparison: {len(comparison_results)} results\")\n",
    "\n",
    "# Reset to original settings\n",
    "exp.LEVELS = LEVELS\n",
    "exp.THINKING_CONDITIONS = THINKING_CONDITIONS\n",
    "exp.N_RUNS = N_RUNS\n",
    "\n",
    "print(\"Custom experiment cell ready. Uncomment code to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
