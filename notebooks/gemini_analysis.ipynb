{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemini 3 Flash Experiment Analysis\n",
    "\n",
    "This notebook analyzes the results of Gemini 3 Flash experiments across moral reasoning benchmarks.\n",
    "\n",
    "**Experiment Design:**\n",
    "- Fixed Chain-of-Thought prompts (level 2)\n",
    "- Varying Gemini's `thinking_level` parameter: minimal, low, medium, high\n",
    "- This isolates the effect of internal reasoning budget from prompt structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Style settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Gemini results\n",
    "results_dir = Path('../results/processed')\n",
    "checkpoint_dir = Path('../results/raw')\n",
    "\n",
    "# Try processed first, then checkpoints\n",
    "def load_gemini_data(benchmark):\n",
    "    processed_path = results_dir / f'gemini_{benchmark}_results.csv'\n",
    "    checkpoint_path = checkpoint_dir / f'gemini_{benchmark}_checkpoint.csv'\n",
    "    \n",
    "    if processed_path.exists():\n",
    "        return pd.read_csv(processed_path)\n",
    "    elif checkpoint_path.exists():\n",
    "        return pd.read_csv(checkpoint_path)\n",
    "    return None\n",
    "\n",
    "ethics_df = load_gemini_data('ethics')\n",
    "moralchoice_df = load_gemini_data('moralchoice')\n",
    "morables_df = load_gemini_data('morables')\n",
    "\n",
    "# Summary\n",
    "print(\"Gemini Experiment Data Summary\")\n",
    "print(\"=\" * 40)\n",
    "for name, df in [('ETHICS', ethics_df), ('MoralChoice', moralchoice_df), ('MORABLES', morables_df)]:\n",
    "    if df is not None:\n",
    "        print(f\"{name}: {len(df)} observations\")\n",
    "        if 'thinking_level' in df.columns:\n",
    "            print(f\"  Thinking levels: {df['thinking_level'].unique().tolist()}\")\n",
    "        if 'run' in df.columns:\n",
    "            print(f\"  Runs: {df['run'].nunique()}\")\n",
    "    else:\n",
    "        print(f\"{name}: No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accuracy by Thinking Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thinking level order for consistent plotting\n",
    "THINKING_LEVEL_ORDER = ['minimal', 'low', 'medium', 'high']\n",
    "\n",
    "def calculate_accuracy_by_thinking_level(df, benchmark_name):\n",
    "    \"\"\"Calculate accuracy statistics by thinking level.\"\"\"\n",
    "    if df is None or 'correct' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Filter valid responses\n",
    "    valid_df = df[df['correct'].notna()].copy()\n",
    "    \n",
    "    # Group by thinking_level\n",
    "    stats = valid_df.groupby('thinking_level').agg(\n",
    "        accuracy=('correct', 'mean'),\n",
    "        n=('correct', 'count'),\n",
    "        std=('correct', 'std')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate standard error\n",
    "    stats['se'] = stats['std'] / np.sqrt(stats['n'])\n",
    "    stats['benchmark'] = benchmark_name\n",
    "    \n",
    "    # Order by thinking level\n",
    "    stats['thinking_level'] = pd.Categorical(\n",
    "        stats['thinking_level'], \n",
    "        categories=THINKING_LEVEL_ORDER, \n",
    "        ordered=True\n",
    "    )\n",
    "    stats = stats.sort_values('thinking_level')\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Calculate for each benchmark\n",
    "ethics_acc = calculate_accuracy_by_thinking_level(ethics_df, 'ETHICS')\n",
    "morables_acc = calculate_accuracy_by_thinking_level(morables_df, 'MORABLES')\n",
    "\n",
    "# Display results\n",
    "print(\"\\nAccuracy by Thinking Level\")\n",
    "print(\"=\" * 50)\n",
    "for name, acc_df in [('ETHICS', ethics_acc), ('MORABLES', morables_acc)]:\n",
    "    if acc_df is not None:\n",
    "        print(f\"\\n{name}:\")\n",
    "        for _, row in acc_df.iterrows():\n",
    "            print(f\"  {row['thinking_level']:8s}: {row['accuracy']:.3f} ± {row['se']:.3f} (n={int(row['n'])})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy by thinking level\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "benchmarks = [\n",
    "    ('ETHICS', ethics_acc, axes[0]),\n",
    "    ('MORABLES', morables_acc, axes[1])\n",
    "]\n",
    "\n",
    "colors = {'minimal': '#e74c3c', 'low': '#f39c12', 'medium': '#3498db', 'high': '#27ae60'}\n",
    "\n",
    "for name, acc_df, ax in benchmarks:\n",
    "    if acc_df is not None:\n",
    "        x = range(len(acc_df))\n",
    "        bars = ax.bar(x, acc_df['accuracy'], \n",
    "                      yerr=acc_df['se'] * 1.96,  # 95% CI\n",
    "                      color=[colors.get(tl, 'gray') for tl in acc_df['thinking_level']],\n",
    "                      capsize=5, alpha=0.8)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(acc_df['thinking_level'], rotation=0)\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax.set_xlabel('Thinking Level')\n",
    "        ax.set_title(f'{name} Accuracy by Thinking Level')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Chance')\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (_, row) in enumerate(acc_df.iterrows()):\n",
    "            ax.text(i, row['accuracy'] + 0.05, f\"{row['accuracy']:.2f}\", \n",
    "                    ha='center', va='bottom', fontsize=10)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'{name} (No data)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/gemini_accuracy_by_thinking_level.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Response Characteristics by Thinking Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_response_characteristics(df, name):\n",
    "    \"\"\"Analyze response length, tokens, and markers by thinking level.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    metrics = ['response_length', 'output_tokens', 'reasoning_markers', 'uncertainty_markers']\n",
    "    available_metrics = [m for m in metrics if m in df.columns]\n",
    "    \n",
    "    if not available_metrics:\n",
    "        return None\n",
    "    \n",
    "    stats = df.groupby('thinking_level')[available_metrics].agg(['mean', 'std']).reset_index()\n",
    "    stats.columns = ['_'.join(col).strip('_') for col in stats.columns]\n",
    "    \n",
    "    # Order by thinking level\n",
    "    stats['thinking_level'] = pd.Categorical(\n",
    "        stats['thinking_level'], \n",
    "        categories=THINKING_LEVEL_ORDER, \n",
    "        ordered=True\n",
    "    )\n",
    "    return stats.sort_values('thinking_level')\n",
    "\n",
    "# Analyze each benchmark\n",
    "for name, df in [('ETHICS', ethics_df), ('MoralChoice', moralchoice_df), ('MORABLES', morables_df)]:\n",
    "    stats = analyze_response_characteristics(df, name)\n",
    "    if stats is not None:\n",
    "        print(f\"\\n{name} Response Characteristics:\")\n",
    "        print(stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot response length distribution by thinking level\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, (name, df) in zip(axes, [('ETHICS', ethics_df), ('MoralChoice', moralchoice_df), ('MORABLES', morables_df)]):\n",
    "    if df is not None and 'response_length' in df.columns:\n",
    "        # Order the data\n",
    "        df_plot = df.copy()\n",
    "        df_plot['thinking_level'] = pd.Categorical(\n",
    "            df_plot['thinking_level'], \n",
    "            categories=THINKING_LEVEL_ORDER, \n",
    "            ordered=True\n",
    "        )\n",
    "        \n",
    "        sns.boxplot(data=df_plot, x='thinking_level', y='response_length', \n",
    "                    ax=ax, palette=colors, order=THINKING_LEVEL_ORDER)\n",
    "        ax.set_title(f'{name} Response Length')\n",
    "        ax.set_xlabel('Thinking Level')\n",
    "        ax.set_ylabel('Response Length (words)')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'{name} (No data)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/gemini_response_length_by_thinking.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_confidence(df, name):\n",
    "    \"\"\"Analyze confidence by thinking level and correctness.\"\"\"\n",
    "    if df is None or 'confidence' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Filter valid confidence scores\n",
    "    valid_df = df[df['confidence'].notna()].copy()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Overall confidence by thinking level\n",
    "    conf_stats = valid_df.groupby('thinking_level').agg(\n",
    "        mean_confidence=('confidence', 'mean'),\n",
    "        std_confidence=('confidence', 'std'),\n",
    "        n=('confidence', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Order\n",
    "    conf_stats['thinking_level'] = pd.Categorical(\n",
    "        conf_stats['thinking_level'], \n",
    "        categories=THINKING_LEVEL_ORDER, \n",
    "        ordered=True\n",
    "    )\n",
    "    \n",
    "    return conf_stats.sort_values('thinking_level')\n",
    "\n",
    "# Analyze confidence\n",
    "print(\"Confidence by Thinking Level\")\n",
    "print(\"=\" * 50)\n",
    "for name, df in [('ETHICS', ethics_df), ('MoralChoice', moralchoice_df), ('MORABLES', morables_df)]:\n",
    "    conf_stats = analyze_confidence(df, name)\n",
    "    if conf_stats is not None:\n",
    "        print(f\"\\n{name}:\")\n",
    "        for _, row in conf_stats.iterrows():\n",
    "            print(f\"  {row['thinking_level']:8s}: {row['mean_confidence']:.1f} ± {row['std_confidence']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence calibration: confidence vs accuracy\n",
    "def plot_calibration(df, name, ax):\n",
    "    \"\"\"Plot confidence calibration by thinking level.\"\"\"\n",
    "    if df is None or 'confidence' not in df.columns or 'correct' not in df.columns:\n",
    "        ax.text(0.5, 0.5, 'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'{name} (No data)')\n",
    "        return\n",
    "    \n",
    "    valid_df = df[df['confidence'].notna() & df['correct'].notna()].copy()\n",
    "    \n",
    "    if len(valid_df) == 0:\n",
    "        ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes)\n",
    "        return\n",
    "    \n",
    "    # Calculate mean confidence and accuracy by thinking level\n",
    "    cal_data = valid_df.groupby('thinking_level').agg(\n",
    "        confidence=('confidence', 'mean'),\n",
    "        accuracy=('correct', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Plot\n",
    "    for _, row in cal_data.iterrows():\n",
    "        ax.scatter(row['confidence'], row['accuracy'] * 100, \n",
    "                   s=150, label=row['thinking_level'],\n",
    "                   color=colors.get(row['thinking_level'], 'gray'),\n",
    "                   edgecolor='black', linewidth=1)\n",
    "    \n",
    "    # Perfect calibration line\n",
    "    ax.plot([0, 100], [0, 100], 'k--', alpha=0.5, label='Perfect calibration')\n",
    "    \n",
    "    ax.set_xlabel('Mean Confidence')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'{name} Calibration')\n",
    "    ax.set_xlim(0, 105)\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for ax, (name, df) in zip(axes, [('ETHICS', ethics_df), ('MoralChoice', moralchoice_df), ('MORABLES', morables_df)]):\n",
    "    plot_calibration(df, name, ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/gemini_calibration.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Token Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tokens(df, name):\n",
    "    \"\"\"Analyze token usage by thinking level.\"\"\"\n",
    "    if df is None:\n",
    "        return None\n",
    "    \n",
    "    token_cols = [c for c in ['input_tokens', 'output_tokens'] if c in df.columns]\n",
    "    if not token_cols:\n",
    "        return None\n",
    "    \n",
    "    stats = df.groupby('thinking_level')[token_cols].agg(['mean', 'sum']).reset_index()\n",
    "    stats.columns = ['_'.join(col).strip('_') for col in stats.columns]\n",
    "    \n",
    "    # Order\n",
    "    stats['thinking_level'] = pd.Categorical(\n",
    "        stats['thinking_level'], \n",
    "        categories=THINKING_LEVEL_ORDER, \n",
    "        ordered=True\n",
    "    )\n",
    "    return stats.sort_values('thinking_level')\n",
    "\n",
    "print(\"Token Usage by Thinking Level\")\n",
    "print(\"=\" * 60)\n",
    "for name, df in [('ETHICS', ethics_df), ('MoralChoice', moralchoice_df), ('MORABLES', morables_df)]:\n",
    "    token_stats = analyze_tokens(df, name)\n",
    "    if token_stats is not None:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(token_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Consistency Across Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_consistency(df, name):\n",
    "    \"\"\"Analyze answer consistency across runs for each item.\"\"\"\n",
    "    if df is None or 'run' not in df.columns or 'extracted_answer' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    # Count unique answers per item per thinking level\n",
    "    consistency = df.groupby(['thinking_level', 'item_id']).agg(\n",
    "        n_runs=('run', 'nunique'),\n",
    "        n_unique_answers=('extracted_answer', 'nunique')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate consistency rate (items where all runs gave same answer)\n",
    "    consistency['consistent'] = consistency['n_unique_answers'] == 1\n",
    "    \n",
    "    summary = consistency.groupby('thinking_level').agg(\n",
    "        consistency_rate=('consistent', 'mean'),\n",
    "        n_items=('item_id', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Order\n",
    "    summary['thinking_level'] = pd.Categorical(\n",
    "        summary['thinking_level'], \n",
    "        categories=THINKING_LEVEL_ORDER, \n",
    "        ordered=True\n",
    "    )\n",
    "    return summary.sort_values('thinking_level')\n",
    "\n",
    "print(\"Consistency Across Runs (Same Answer Rate)\")\n",
    "print(\"=\" * 50)\n",
    "for name, df in [('ETHICS', ethics_df), ('MoralChoice', moralchoice_df), ('MORABLES', morables_df)]:\n",
    "    cons = analyze_consistency(df, name)\n",
    "    if cons is not None:\n",
    "        print(f\"\\n{name}:\")\n",
    "        for _, row in cons.iterrows():\n",
    "            print(f\"  {row['thinking_level']:8s}: {row['consistency_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "def create_summary_table():\n",
    "    \"\"\"Create a comprehensive summary table.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for name, df in [('ETHICS', ethics_df), ('MoralChoice', moralchoice_df), ('MORABLES', morables_df)]:\n",
    "        if df is None:\n",
    "            continue\n",
    "            \n",
    "        for tl in THINKING_LEVEL_ORDER:\n",
    "            tl_df = df[df['thinking_level'] == tl]\n",
    "            if len(tl_df) == 0:\n",
    "                continue\n",
    "            \n",
    "            row = {\n",
    "                'Benchmark': name,\n",
    "                'Thinking Level': tl,\n",
    "                'N': len(tl_df),\n",
    "            }\n",
    "            \n",
    "            # Accuracy (if applicable)\n",
    "            if 'correct' in tl_df.columns:\n",
    "                valid = tl_df[tl_df['correct'].notna()]\n",
    "                if len(valid) > 0:\n",
    "                    row['Accuracy'] = f\"{valid['correct'].mean():.3f}\"\n",
    "            \n",
    "            # Confidence\n",
    "            if 'confidence' in tl_df.columns:\n",
    "                conf_valid = tl_df[tl_df['confidence'].notna()]\n",
    "                if len(conf_valid) > 0:\n",
    "                    row['Mean Confidence'] = f\"{conf_valid['confidence'].mean():.1f}\"\n",
    "            \n",
    "            # Response length\n",
    "            if 'response_length' in tl_df.columns:\n",
    "                row['Avg Response Length'] = f\"{tl_df['response_length'].mean():.0f}\"\n",
    "            \n",
    "            # Output tokens\n",
    "            if 'output_tokens' in tl_df.columns:\n",
    "                row['Avg Output Tokens'] = f\"{tl_df['output_tokens'].mean():.0f}\"\n",
    "            \n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "summary_table = create_summary_table()\n",
    "print(\"\\nGemini Experiment Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary table\n",
    "if len(summary_table) > 0:\n",
    "    summary_table.to_csv('../outputs/gemini_summary.csv', index=False)\n",
    "    print(\"Summary saved to ../outputs/gemini_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Findings\n",
    "\n",
    "Based on the analysis above, summarize the key findings:\n",
    "\n",
    "1. **Effect of Thinking Level on Accuracy**: Does increasing thinking_level improve accuracy?\n",
    "2. **Response Characteristics**: How do response length and reasoning markers change with thinking level?\n",
    "3. **Confidence Calibration**: Is Gemini well-calibrated? Does this vary by thinking level?\n",
    "4. **Consistency**: Are higher thinking levels more consistent across runs?\n",
    "5. **Cost-Benefit**: Is the additional token cost of higher thinking levels justified by accuracy gains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key findings programmatically\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if accuracy improves with thinking level\n",
    "for name, df in [('ETHICS', ethics_df), ('MORABLES', morables_df)]:\n",
    "    if df is not None and 'correct' in df.columns:\n",
    "        valid = df[df['correct'].notna()]\n",
    "        if len(valid) > 0:\n",
    "            acc_by_tl = valid.groupby('thinking_level')['correct'].mean()\n",
    "            \n",
    "            # Check if ordered\n",
    "            minimal = acc_by_tl.get('minimal', 0)\n",
    "            high = acc_by_tl.get('high', 0)\n",
    "            \n",
    "            if high > minimal:\n",
    "                print(f\"\\n{name}: Higher thinking levels improve accuracy\")\n",
    "                print(f\"  minimal -> high: {minimal:.3f} -> {high:.3f} (+{(high-minimal)*100:.1f}pp)\")\n",
    "            elif high < minimal:\n",
    "                print(f\"\\n{name}: Higher thinking levels decrease accuracy\")\n",
    "                print(f\"  minimal -> high: {minimal:.3f} -> {high:.3f} ({(high-minimal)*100:.1f}pp)\")\n",
    "            else:\n",
    "                print(f\"\\n{name}: No clear effect of thinking level on accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
