{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Consistency Analysis: Variance Across Runs\n",
    "\n",
    "This notebook analyzes the consistency of model responses across multiple runs (N=3) for each condition.\n",
    "\n",
    "**Key Questions:**\n",
    "1. How consistent are answers across runs? (Same item, same condition, different runs)\n",
    "2. Does reflection level affect response consistency?\n",
    "3. How variable is confidence across runs?\n",
    "4. Are certain subscales/categories more consistent than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Colors\n",
    "LEVEL_COLORS = {0: '#2ecc71', 2: '#3498db', 4: '#9b59b6', 5: '#e74c3c'}\n",
    "THINKING_COLORS = {False: '#27ae60', True: '#c0392b'}\n",
    "\n",
    "# Create output directory\n",
    "Path('../outputs/figures').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "results_dir = Path(\"../results/raw\")\n",
    "\n",
    "ethics_df = pd.read_csv(results_dir / \"ethics_checkpoint.csv\") if (results_dir / \"ethics_checkpoint.csv\").exists() else None\n",
    "mc_df = pd.read_csv(results_dir / \"moralchoice_checkpoint.csv\") if (results_dir / \"moralchoice_checkpoint.csv\").exists() else None\n",
    "morables_df = pd.read_csv(results_dir / \"morables_checkpoint.csv\") if (results_dir / \"morables_checkpoint.csv\").exists() else None\n",
    "\n",
    "print(\"Data Loaded:\")\n",
    "for name, df in [('ETHICS', ethics_df), ('MoralChoice', mc_df), ('MORABLES', morables_df)]:\n",
    "    if df is not None:\n",
    "        n_items = df['item_id'].nunique()\n",
    "        n_runs = df['run'].nunique()\n",
    "        print(f\"  {name}: {len(df):,} obs, {n_items} items, {n_runs} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Answer Consistency Across Runs\n",
    "\n",
    "For each item × level × thinking condition, how often does the model give the **same answer** across all 3 runs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consistency(df):\n",
    "    \"\"\"Calculate % of items with identical answers across all runs.\"\"\"\n",
    "    if df is None or 'extracted_answer' not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    valid = df[df['extracted_answer'].notna()]\n",
    "    \n",
    "    # Group by item, level, thinking and count unique answers\n",
    "    consistency = valid.groupby(['item_id', 'level', 'thinking']).agg(\n",
    "        n_unique_answers=('extracted_answer', 'nunique'),\n",
    "        n_runs=('run', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Consistent = only 1 unique answer across all runs\n",
    "    consistency['is_consistent'] = consistency['n_unique_answers'] == 1\n",
    "    \n",
    "    return consistency\n",
    "\n",
    "# Calculate for each benchmark\n",
    "ethics_cons = calculate_consistency(ethics_df)\n",
    "mc_cons = calculate_consistency(mc_df)\n",
    "morables_cons = calculate_consistency(morables_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Answer Consistency by Reflection Level\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (cons, name) in zip(axes, [(ethics_cons, 'ETHICS'), (mc_cons, 'MoralChoice'), (morables_cons, 'MORABLES')]):\n",
    "    if cons is None:\n",
    "        ax.text(0.5, 0.5, f'No data for {name}', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(name)\n",
    "        continue\n",
    "    \n",
    "    # Calculate consistency rate by level\n",
    "    cons_by_level = cons.groupby('level')['is_consistent'].mean() * 100\n",
    "    \n",
    "    bars = ax.bar(cons_by_level.index, cons_by_level.values, \n",
    "                  color=[LEVEL_COLORS.get(l, 'gray') for l in cons_by_level.index],\n",
    "                  edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, cons_by_level.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level')\n",
    "    ax.set_ylabel('Consistency Rate (%)')\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.set_xticks(cons_by_level.index)\n",
    "    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Answer Consistency Across Runs by Reflection Level\\n(% of items with identical answers in all 3 runs)', \n",
    "             fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/consistency_fig1_by_level.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Consistency by Level and Thinking Mode\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (cons, name) in zip(axes, [(ethics_cons, 'ETHICS'), (mc_cons, 'MoralChoice'), (morables_cons, 'MORABLES')]):\n",
    "    if cons is None:\n",
    "        ax.text(0.5, 0.5, f'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(name)\n",
    "        continue\n",
    "    \n",
    "    levels = sorted(cons['level'].unique())\n",
    "    x = np.arange(len(levels))\n",
    "    width = 0.35\n",
    "    \n",
    "    for i, (thinking, color, label) in enumerate([(False, THINKING_COLORS[False], 'No Thinking'),\n",
    "                                                   (True, THINKING_COLORS[True], 'With Thinking')]):\n",
    "        subset = cons[cons['thinking'] == thinking]\n",
    "        rates = [subset[subset['level'] == l]['is_consistent'].mean() * 100 for l in levels]\n",
    "        \n",
    "        bars = ax.bar(x + (i - 0.5) * width, rates, width, label=label, color=color, alpha=0.8)\n",
    "        \n",
    "        # Value labels\n",
    "        for bar, val in zip(bars, rates):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{val:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level')\n",
    "    ax.set_ylabel('Consistency Rate (%)')\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(levels)\n",
    "    ax.set_ylim(0, 110)\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "plt.suptitle('Answer Consistency: Thinking Mode × Reflection Level', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/consistency_fig2_thinking_interaction.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Heatmap of Consistency Across All Conditions\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (cons, name) in zip(axes, [(ethics_cons, 'ETHICS'), (mc_cons, 'MoralChoice'), (morables_cons, 'MORABLES')]):\n",
    "    if cons is None:\n",
    "        ax.text(0.5, 0.5, f'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(name)\n",
    "        continue\n",
    "    \n",
    "    # Pivot for heatmap\n",
    "    pivot = cons.groupby(['level', 'thinking'])['is_consistent'].mean().unstack() * 100\n",
    "    pivot.columns = ['No Thinking', 'With Thinking']\n",
    "    \n",
    "    sns.heatmap(pivot, annot=True, fmt='.1f', cmap='RdYlGn', \n",
    "                center=80, vmin=50, vmax=100, ax=ax,\n",
    "                cbar_kws={'label': 'Consistency %'})\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_xlabel('Thinking Mode')\n",
    "    ax.set_ylabel('Reflection Level')\n",
    "\n",
    "plt.suptitle('Answer Consistency Heatmap (% identical across 3 runs)', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/consistency_fig3_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Accuracy Variance Across Runs\n",
    "\n",
    "How much does accuracy vary across the 3 runs for each condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Accuracy Variance by Run\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(ethics_df, 'ETHICS'), (morables_df, 'MORABLES')]):\n",
    "    if df is None or 'correct' not in df.columns:\n",
    "        ax.text(0.5, 0.5, f'No data for {name}', ha='center', va='center', transform=ax.transAxes)\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['correct'].notna()]\n",
    "    \n",
    "    # Calculate accuracy by level, thinking, and run\n",
    "    acc_by_run = valid.groupby(['level', 'thinking', 'run'])['correct'].mean() * 100\n",
    "    acc_by_run = acc_by_run.reset_index()\n",
    "    \n",
    "    # Plot each level\n",
    "    levels = sorted(valid['level'].unique())\n",
    "    \n",
    "    for level in levels:\n",
    "        level_data = acc_by_run[acc_by_run['level'] == level]\n",
    "        \n",
    "        # Aggregate across thinking for simplicity\n",
    "        by_run = level_data.groupby('run')['correct'].mean()\n",
    "        \n",
    "        ax.plot(by_run.index, by_run.values, marker='o', markersize=8,\n",
    "                linewidth=2, color=LEVEL_COLORS.get(level, 'gray'),\n",
    "                label=f'Level {level}')\n",
    "    \n",
    "    ax.set_xlabel('Run Number')\n",
    "    ax.set_ylabel('Accuracy (%)')\n",
    "    ax.set_title(f'{name}: Accuracy by Run')\n",
    "    ax.set_xticks(valid['run'].unique())\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(40, 100)\n",
    "\n",
    "plt.suptitle('Accuracy Stability Across Runs', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/consistency_fig4_accuracy_by_run.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: Accuracy Standard Deviation by Condition\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(ethics_df, 'ETHICS'), (morables_df, 'MORABLES')]):\n",
    "    if df is None or 'correct' not in df.columns:\n",
    "        ax.text(0.5, 0.5, f'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['correct'].notna()]\n",
    "    \n",
    "    # Calculate per-item accuracy variance across runs\n",
    "    item_variance = valid.groupby(['item_id', 'level', 'thinking']).agg(\n",
    "        mean_correct=('correct', 'mean'),\n",
    "        std_correct=('correct', 'std'),\n",
    "        n_runs=('run', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Mean std by level (items with variance > 0 means inconsistent)\n",
    "    # For binary outcomes, std > 0 means not all runs agreed\n",
    "    item_variance['has_variance'] = item_variance['std_correct'] > 0\n",
    "    \n",
    "    variance_rate = item_variance.groupby('level')['has_variance'].mean() * 100\n",
    "    \n",
    "    bars = ax.bar(variance_rate.index, variance_rate.values,\n",
    "                  color=[LEVEL_COLORS.get(l, 'gray') for l in variance_rate.index],\n",
    "                  edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    for bar, val in zip(bars, variance_rate.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level')\n",
    "    ax.set_ylabel('% Items with Answer Variance')\n",
    "    ax.set_title(f'{name}: Items with Inconsistent Answers Across Runs')\n",
    "    ax.set_xticks(variance_rate.index)\n",
    "    ax.set_ylim(0, max(variance_rate.values) * 1.3 if len(variance_rate) > 0 else 50)\n",
    "\n",
    "plt.suptitle('Answer Variance Rate by Reflection Level\\n(% of items where accuracy differed across runs)', \n",
    "             fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/consistency_fig5_variance_rate.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Confidence Variance Across Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 6: Confidence Variance by Reflection Level\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(ethics_df, 'ETHICS'), (mc_df, 'MoralChoice'), (morables_df, 'MORABLES')]):\n",
    "    if df is None or 'confidence' not in df.columns:\n",
    "        ax.text(0.5, 0.5, f'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(name)\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['confidence'].notna()]\n",
    "    \n",
    "    # Calculate per-item confidence std across runs\n",
    "    conf_var = valid.groupby(['item_id', 'level', 'thinking']).agg(\n",
    "        mean_conf=('confidence', 'mean'),\n",
    "        std_conf=('confidence', 'std'),\n",
    "        n_runs=('run', 'count')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Fill NaN std (when only 1 run) with 0\n",
    "    conf_var['std_conf'] = conf_var['std_conf'].fillna(0)\n",
    "    \n",
    "    # Box plot of std by level\n",
    "    levels = sorted(conf_var['level'].unique())\n",
    "    data = [conf_var[conf_var['level'] == l]['std_conf'].values for l in levels]\n",
    "    \n",
    "    bp = ax.boxplot(data, positions=levels, widths=0.6, patch_artist=True)\n",
    "    \n",
    "    for patch, level in zip(bp['boxes'], levels):\n",
    "        patch.set_facecolor(LEVEL_COLORS.get(level, 'gray'))\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level')\n",
    "    ax.set_ylabel('Confidence Std Dev (across runs)')\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_xticks(levels)\n",
    "\n",
    "plt.suptitle('Confidence Variability Across Runs by Reflection Level', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/consistency_fig6_confidence_variance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 7: Mean Confidence Std by Condition (Bar Chart)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(ethics_df, 'ETHICS'), (mc_df, 'MoralChoice'), (morables_df, 'MORABLES')]):\n",
    "    if df is None or 'confidence' not in df.columns:\n",
    "        ax.text(0.5, 0.5, f'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(name)\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['confidence'].notna()]\n",
    "    \n",
    "    # Calculate per-item confidence std\n",
    "    conf_var = valid.groupby(['item_id', 'level', 'thinking'])['confidence'].std().reset_index()\n",
    "    conf_var.columns = ['item_id', 'level', 'thinking', 'std_conf']\n",
    "    conf_var['std_conf'] = conf_var['std_conf'].fillna(0)\n",
    "    \n",
    "    # Mean std by level and thinking\n",
    "    mean_std = conf_var.groupby(['level', 'thinking'])['std_conf'].mean().unstack()\n",
    "    mean_std.columns = ['No Thinking', 'With Thinking']\n",
    "    \n",
    "    mean_std.plot(kind='bar', ax=ax, color=[THINKING_COLORS[False], THINKING_COLORS[True]], \n",
    "                  edgecolor='black', linewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('Reflection Level')\n",
    "    ax.set_ylabel('Mean Confidence Std Dev')\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.legend(title='Thinking')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.suptitle('Mean Confidence Standard Deviation Across Runs', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/consistency_fig7_mean_conf_std.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8: Confidence by Run (Line Plot)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for ax, (df, name) in zip(axes, [(ethics_df, 'ETHICS'), (mc_df, 'MoralChoice'), (morables_df, 'MORABLES')]):\n",
    "    if df is None or 'confidence' not in df.columns:\n",
    "        ax.text(0.5, 0.5, f'No data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(name)\n",
    "        continue\n",
    "    \n",
    "    valid = df[df['confidence'].notna()]\n",
    "    \n",
    "    for level in sorted(valid['level'].unique()):\n",
    "        level_data = valid[valid['level'] == level]\n",
    "        conf_by_run = level_data.groupby('run')['confidence'].agg(['mean', 'std'])\n",
    "        \n",
    "        ax.errorbar(conf_by_run.index, conf_by_run['mean'], yerr=conf_by_run['std'],\n",
    "                    marker='o', markersize=8, linewidth=2, capsize=4,\n",
    "                    color=LEVEL_COLORS.get(level, 'gray'), label=f'Level {level}')\n",
    "    \n",
    "    ax.set_xlabel('Run Number')\n",
    "    ax.set_ylabel('Mean Confidence (± std)')\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_xticks(valid['run'].unique())\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.set_ylim(50, 100)\n",
    "\n",
    "plt.suptitle('Confidence Stability Across Runs', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/figures/consistency_fig8_confidence_by_run.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Consistency by Subscale/Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 9: ETHICS Consistency by Subscale\n",
    "\n",
    "if ethics_df is not None and 'subscale' in ethics_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Recalculate consistency with subscale\n",
    "    valid = ethics_df[ethics_df['extracted_answer'].notna()]\n",
    "    \n",
    "    cons = valid.groupby(['item_id', 'level', 'thinking', 'subscale']).agg(\n",
    "        n_unique=('extracted_answer', 'nunique')\n",
    "    ).reset_index()\n",
    "    cons['is_consistent'] = cons['n_unique'] == 1\n",
    "    \n",
    "    # Consistency by subscale and level\n",
    "    subscales = ['commonsense', 'deontology', 'virtue']\n",
    "    levels = sorted(cons['level'].unique())\n",
    "    x = np.arange(len(subscales))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, level in enumerate(levels):\n",
    "        rates = [cons[(cons['subscale'] == s) & (cons['level'] == level)]['is_consistent'].mean() * 100 \n",
    "                 for s in subscales]\n",
    "        bars = ax.bar(x + i*width, rates, width, label=f'Level {level}', \n",
    "                      color=LEVEL_COLORS.get(level, 'gray'), edgecolor='black')\n",
    "        \n",
    "        for bar, val in zip(bars, rates):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{val:.0f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Subscale')\n",
    "    ax.set_ylabel('Consistency Rate (%)')\n",
    "    ax.set_title('ETHICS: Answer Consistency by Subscale and Reflection Level')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels([s.capitalize() for s in subscales])\n",
    "    ax.legend(title='Reflection Level')\n",
    "    ax.set_ylim(0, 110)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/consistency_fig9_ethics_subscale.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"ETHICS subscale data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 10: MoralChoice Consistency by Ambiguity\n",
    "\n",
    "if mc_df is not None and 'ambiguity' in mc_df.columns:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    valid = mc_df[mc_df['extracted_answer'].notna()]\n",
    "    \n",
    "    cons = valid.groupby(['item_id', 'level', 'thinking', 'ambiguity']).agg(\n",
    "        n_unique=('extracted_answer', 'nunique')\n",
    "    ).reset_index()\n",
    "    cons['is_consistent'] = cons['n_unique'] == 1\n",
    "    \n",
    "    # Consistency by ambiguity and level\n",
    "    ambiguities = ['low', 'high']\n",
    "    levels = sorted(cons['level'].unique())\n",
    "    x = np.arange(len(ambiguities))\n",
    "    width = 0.2\n",
    "    \n",
    "    for i, level in enumerate(levels):\n",
    "        rates = [cons[(cons['ambiguity'] == a) & (cons['level'] == level)]['is_consistent'].mean() * 100 \n",
    "                 for a in ambiguities]\n",
    "        bars = ax.bar(x + i*width, rates, width, label=f'Level {level}',\n",
    "                      color=LEVEL_COLORS.get(level, 'gray'), edgecolor='black')\n",
    "        \n",
    "        for bar, val in zip(bars, rates):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                    f'{val:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Ambiguity Level')\n",
    "    ax.set_ylabel('Consistency Rate (%)')\n",
    "    ax.set_title('MoralChoice: Answer Consistency by Ambiguity and Reflection Level')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(['Low Ambiguity', 'High Ambiguity'])\n",
    "    ax.legend(title='Reflection Level')\n",
    "    ax.set_ylim(0, 110)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/figures/consistency_fig10_mc_ambiguity.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"MoralChoice ambiguity data not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Table: Consistency Metrics\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for df, name, cons in [(ethics_df, 'ETHICS', ethics_cons), \n",
    "                        (mc_df, 'MoralChoice', mc_cons), \n",
    "                        (morables_df, 'MORABLES', morables_cons)]:\n",
    "    if df is None or cons is None:\n",
    "        continue\n",
    "    \n",
    "    for level in sorted(df['level'].unique()):\n",
    "        for thinking in [False, True]:\n",
    "            subset_cons = cons[(cons['level'] == level) & (cons['thinking'] == thinking)]\n",
    "            \n",
    "            row = {\n",
    "                'Benchmark': name,\n",
    "                'Level': level,\n",
    "                'Thinking': 'On' if thinking else 'Off',\n",
    "                'N_Items': len(subset_cons),\n",
    "                'Consistency %': round(subset_cons['is_consistent'].mean() * 100, 1),\n",
    "            }\n",
    "            \n",
    "            # Add confidence variance if available\n",
    "            if 'confidence' in df.columns:\n",
    "                subset_df = df[(df['level'] == level) & (df['thinking'] == thinking) & df['confidence'].notna()]\n",
    "                if len(subset_df) > 0:\n",
    "                    conf_std = subset_df.groupby('item_id')['confidence'].std().mean()\n",
    "                    row['Mean Conf Std'] = round(conf_std, 2) if not pd.isna(conf_std) else 0\n",
    "            \n",
    "            summary_data.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Consistency Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Save\n",
    "summary_df.to_csv('../outputs/consistency_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Findings\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"KEY CONSISTENCY FINDINGS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for cons, name in [(ethics_cons, 'ETHICS'), (mc_cons, 'MoralChoice'), (morables_cons, 'MORABLES')]:\n",
    "    if cons is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    # Overall consistency\n",
    "    overall = cons['is_consistent'].mean() * 100\n",
    "    print(f\"  Overall consistency: {overall:.1f}%\")\n",
    "    \n",
    "    # Best and worst levels\n",
    "    by_level = cons.groupby('level')['is_consistent'].mean() * 100\n",
    "    best_level = by_level.idxmax()\n",
    "    worst_level = by_level.idxmin()\n",
    "    print(f\"  Most consistent: Level {best_level} ({by_level[best_level]:.1f}%)\")\n",
    "    print(f\"  Least consistent: Level {worst_level} ({by_level[worst_level]:.1f}%)\")\n",
    "    \n",
    "    # Thinking effect\n",
    "    no_think = cons[cons['thinking'] == False]['is_consistent'].mean() * 100\n",
    "    with_think = cons[cons['thinking'] == True]['is_consistent'].mean() * 100\n",
    "    print(f\"  No thinking: {no_think:.1f}% | With thinking: {with_think:.1f}% (Δ = {with_think - no_think:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure list\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIGURES GENERATED\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "figures = [\n",
    "    (\"consistency_fig1_by_level.png\", \"Answer consistency by reflection level\"),\n",
    "    (\"consistency_fig2_thinking_interaction.png\", \"Consistency: thinking × level interaction\"),\n",
    "    (\"consistency_fig3_heatmap.png\", \"Consistency heatmap across all conditions\"),\n",
    "    (\"consistency_fig4_accuracy_by_run.png\", \"Accuracy stability across runs\"),\n",
    "    (\"consistency_fig5_variance_rate.png\", \"Answer variance rate by level\"),\n",
    "    (\"consistency_fig6_confidence_variance.png\", \"Confidence variance box plots\"),\n",
    "    (\"consistency_fig7_mean_conf_std.png\", \"Mean confidence std by condition\"),\n",
    "    (\"consistency_fig8_confidence_by_run.png\", \"Confidence stability across runs\"),\n",
    "    (\"consistency_fig9_ethics_subscale.png\", \"ETHICS consistency by subscale\"),\n",
    "    (\"consistency_fig10_mc_ambiguity.png\", \"MoralChoice consistency by ambiguity\"),\n",
    "]\n",
    "\n",
    "for fname, desc in figures:\n",
    "    path = Path(f'../outputs/figures/{fname}')\n",
    "    status = \"✓\" if path.exists() else \"○\"\n",
    "    print(f\"  {status} {fname}: {desc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
