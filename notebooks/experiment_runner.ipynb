{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Experiment Runner\n",
    "\n",
    "This notebook allows you to run reflection experiments with configurable parameters.\n",
    "\n",
    "**Configurable Parameters:**\n",
    "- Number of runs\n",
    "- Reflection levels (0-5)\n",
    "- Thinking conditions (ON/OFF)\n",
    "- Dataset selection (ETHICS, MoralChoice, or both)\n",
    "- Sample size\n",
    "- Model settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "# Load environment variables\n",
    "if os.path.exists('.env'):\n",
    "    with open('.env', 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith('#'):\n",
    "                match = re.match(r'(\\w+)\\s*=\\s*[\"\\']?([^\"\\']+)[\"\\']?', line)\n",
    "                if match:\n",
    "                    os.environ[match.group(1)] = match.group(2)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project imports\n",
    "import config\n",
    "from prompts import get_ethics_prompt, get_moralchoice_prompt\n",
    "from src.api import call_with_rate_limit, call_claude, APIResponse\n",
    "from src.extraction import (\n",
    "    extract_ethics_answer,\n",
    "    extract_moralchoice_with_confidence,\n",
    "    count_reasoning_markers,\n",
    "    count_uncertainty_markers\n",
    ")\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Model: {config.MODEL}\")\n",
    "print(f\"API Key loaded: {'Yes' if config.ANTHROPIC_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration\n",
    "\n",
    "**Modify these parameters to customize your experiment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT CONFIGURATION - MODIFY THESE PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Reflection levels to test (0-5)\n",
    "# Level 0: Direct answer only\n",
    "# Level 1: Minimal reasoning\n",
    "# Level 2: Chain-of-thought\n",
    "# Level 3: Structured reasoning\n",
    "# Level 4: Adversarial (consider counterarguments)\n",
    "# Level 5: Two-pass reflection\n",
    "LEVELS = [0, 2, 4]  # Example: test direct, CoT, and adversarial\n",
    "\n",
    "# Thinking conditions\n",
    "THINKING_CONDITIONS = [False, True]  # [OFF, ON] or just [False] or [True]\n",
    "\n",
    "# Number of runs per condition (for consistency measurement)\n",
    "N_RUNS = 1  # Set to 3 for full consistency analysis\n",
    "\n",
    "# Dataset selection\n",
    "RUN_ETHICS = True\n",
    "RUN_MORALCHOICE = True\n",
    "\n",
    "# Sample size (set to None for full dataset)\n",
    "SAMPLE_SIZE = 10  # Start small for testing\n",
    "\n",
    "# Include confidence in MoralChoice prompts\n",
    "INCLUDE_CONFIDENCE = True\n",
    "\n",
    "# Rate limiting (calls per minute)\n",
    "RATE_LIMIT = 50\n",
    "\n",
    "# Output settings\n",
    "SAVE_CHECKPOINTS = True\n",
    "CHECKPOINT_DIR = \"results/notebook\"\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Calculate estimated API calls\n",
    "def estimate_calls():\n",
    "    ethics_items = SAMPLE_SIZE if SAMPLE_SIZE else 1500\n",
    "    mc_items = SAMPLE_SIZE if SAMPLE_SIZE else 500\n",
    "    \n",
    "    # Level 5 requires 2 calls per item\n",
    "    calls_per_item = sum(2 if l == 5 else 1 for l in LEVELS)\n",
    "    calls_per_condition = calls_per_item * len(THINKING_CONDITIONS) * N_RUNS\n",
    "    \n",
    "    ethics_calls = ethics_items * calls_per_condition if RUN_ETHICS else 0\n",
    "    mc_calls = mc_items * calls_per_condition if RUN_MORALCHOICE else 0\n",
    "    \n",
    "    return ethics_calls, mc_calls\n",
    "\n",
    "ethics_calls, mc_calls = estimate_calls()\n",
    "total_calls = ethics_calls + mc_calls\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EXPERIMENT CONFIGURATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Levels: {LEVELS}\")\n",
    "print(f\"Thinking conditions: {['OFF' if not t else 'ON' for t in THINKING_CONDITIONS]}\")\n",
    "print(f\"Runs per condition: {N_RUNS}\")\n",
    "print(f\"Sample size: {SAMPLE_SIZE or 'Full dataset'}\")\n",
    "print(f\"\\nDatasets:\")\n",
    "print(f\"  ETHICS: {'Yes' if RUN_ETHICS else 'No'}\")\n",
    "print(f\"  MoralChoice: {'Yes' if RUN_MORALCHOICE else 'No'}\")\n",
    "print(f\"\\nEstimated API calls: ~{total_calls}\")\n",
    "print(f\"Estimated time at {RATE_LIMIT} calls/min: ~{total_calls / RATE_LIMIT:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ethics_data(sample_size=None):\n",
    "    \"\"\"Load ETHICS dataset with optional balanced sampling.\"\"\"\n",
    "    ethics = pd.read_csv(\"data/ethics_sample.csv\")\n",
    "    \n",
    "    if sample_size:\n",
    "        per_subscale = sample_size // 3\n",
    "        subscales = []\n",
    "        for subscale in ['commonsense', 'deontology', 'virtue']:\n",
    "            subset = ethics[ethics['subscale'] == subscale].head(per_subscale)\n",
    "            subscales.append(subset)\n",
    "        ethics = pd.concat(subscales, ignore_index=True)\n",
    "    \n",
    "    return ethics\n",
    "\n",
    "def load_moralchoice_data(sample_size=None):\n",
    "    \"\"\"Load MoralChoice dataset with optional balanced sampling.\"\"\"\n",
    "    mc = pd.read_csv(\"data/moralchoice_sample.csv\")\n",
    "    \n",
    "    if sample_size:\n",
    "        per_level = sample_size // 2\n",
    "        low_amb = mc[mc['ambiguity'] == 'low'].head(per_level)\n",
    "        high_amb = mc[mc['ambiguity'] == 'high'].head(per_level)\n",
    "        mc = pd.concat([low_amb, high_amb], ignore_index=True)\n",
    "    \n",
    "    return mc\n",
    "\n",
    "# Load datasets\n",
    "if RUN_ETHICS:\n",
    "    ethics_data = load_ethics_data(SAMPLE_SIZE)\n",
    "    print(f\"ETHICS loaded: {len(ethics_data)} items\")\n",
    "    print(f\"  Subscales: {ethics_data['subscale'].value_counts().to_dict()}\")\n",
    "\n",
    "if RUN_MORALCHOICE:\n",
    "    mc_data = load_moralchoice_data(SAMPLE_SIZE)\n",
    "    print(f\"\\nMoralChoice loaded: {len(mc_data)} items\")\n",
    "    if 'ambiguity' in mc_data.columns:\n",
    "        print(f\"  Ambiguity: {mc_data['ambiguity'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preview Data & Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview sample items\n",
    "if RUN_ETHICS:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ETHICS SAMPLE\")\n",
    "    print(\"=\" * 50)\n",
    "    sample_ethics = ethics_data.iloc[0]\n",
    "    print(f\"Scenario: {sample_ethics['scenario'][:200]}...\")\n",
    "    print(f\"Label: {sample_ethics['label']}\")\n",
    "    print(f\"Subscale: {sample_ethics['subscale']}\")\n",
    "\n",
    "if RUN_MORALCHOICE:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"MORALCHOICE SAMPLE\")\n",
    "    print(\"=\" * 50)\n",
    "    sample_mc = mc_data.iloc[0]\n",
    "    print(f\"Context: {sample_mc.get('context', 'N/A')[:100]}...\")\n",
    "    print(f\"Option A: {sample_mc['option_a'][:100]}...\")\n",
    "    print(f\"Option B: {sample_mc['option_b'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview prompts at different levels\n",
    "def preview_prompts(dataset='ethics', levels=[0, 2, 5]):\n",
    "    \"\"\"Preview how prompts look at different levels.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT PREVIEW - {dataset.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if dataset == 'ethics' and RUN_ETHICS:\n",
    "        sample = ethics_data.iloc[0]\n",
    "        for level in levels:\n",
    "            print(f\"\\n--- Level {level} ---\")\n",
    "            prompt = get_ethics_prompt(level, sample['scenario'])\n",
    "            print(prompt[:500] + \"...\" if len(prompt) > 500 else prompt)\n",
    "    \n",
    "    elif dataset == 'moralchoice' and RUN_MORALCHOICE:\n",
    "        sample = mc_data.iloc[0]\n",
    "        for level in levels:\n",
    "            print(f\"\\n--- Level {level} ---\")\n",
    "            prompt = get_moralchoice_prompt(\n",
    "                level, \n",
    "                sample.get('context', ''),\n",
    "                sample['option_a'],\n",
    "                sample['option_b'],\n",
    "                include_confidence=INCLUDE_CONFIDENCE\n",
    "            )\n",
    "            print(prompt[:500] + \"...\" if len(prompt) > 500 else prompt)\n",
    "\n",
    "# Preview prompts for configured levels\n",
    "preview_prompts('ethics', LEVELS[:3])  # Show first 3 levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Core Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_ethics(row, level, thinking, rate_limit=True):\n",
    "    \"\"\"\n",
    "    Run single ETHICS item at given condition.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with scenario, label\n",
    "        level: Reflection level (0-5)\n",
    "        thinking: Whether to enable extended thinking\n",
    "        rate_limit: Whether to apply rate limiting\n",
    "    \n",
    "    Returns:\n",
    "        Dict with response data and metrics\n",
    "    \"\"\"\n",
    "    api_call = call_with_rate_limit if rate_limit else call_claude\n",
    "    \n",
    "    if level == 5:\n",
    "        # Two-pass reflection\n",
    "        prompt1 = get_ethics_prompt(5, row['scenario'])\n",
    "        response1 = api_call(prompt1, thinking)\n",
    "        \n",
    "        prompt2 = get_ethics_prompt(5, row['scenario'], response1.content)\n",
    "        response2 = api_call(prompt2, thinking)\n",
    "        \n",
    "        content = response2.content\n",
    "        full_response = f\"[PASS1]\\n{response1.content}\\n\\n[PASS2]\\n{response2.content}\"\n",
    "        thinking_content = response2.thinking\n",
    "        input_tokens = response1.input_tokens + response2.input_tokens\n",
    "        output_tokens = response1.output_tokens + response2.output_tokens\n",
    "    else:\n",
    "        prompt = get_ethics_prompt(level, row['scenario'])\n",
    "        response = api_call(prompt, thinking)\n",
    "        \n",
    "        content = response.content\n",
    "        full_response = response.content\n",
    "        thinking_content = response.thinking\n",
    "        input_tokens = response.input_tokens\n",
    "        output_tokens = response.output_tokens\n",
    "    \n",
    "    # Extract answer\n",
    "    extracted = extract_ethics_answer(content)\n",
    "    \n",
    "    return {\n",
    "        'item_id': row['item_id'],\n",
    "        'subscale': row['subscale'],\n",
    "        'scenario': row['scenario'][:200],\n",
    "        'correct_answer': row['label'],\n",
    "        'level': level,\n",
    "        'thinking': thinking,\n",
    "        'response': full_response,\n",
    "        'thinking_content': thinking_content,\n",
    "        'extracted_answer': extracted,\n",
    "        'correct': extracted == row['label'] if extracted else None,\n",
    "        'response_length': len(full_response.split()),\n",
    "        'reasoning_markers': count_reasoning_markers(full_response),\n",
    "        'uncertainty_markers': count_uncertainty_markers(full_response),\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "\n",
    "def run_single_moralchoice(row, level, thinking, include_confidence=True, rate_limit=True):\n",
    "    \"\"\"\n",
    "    Run single MoralChoice item at given condition.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with context, option_a, option_b\n",
    "        level: Reflection level (0-5)\n",
    "        thinking: Whether to enable extended thinking\n",
    "        include_confidence: Whether to ask for confidence score\n",
    "        rate_limit: Whether to apply rate limiting\n",
    "    \n",
    "    Returns:\n",
    "        Dict with response data and metrics\n",
    "    \"\"\"\n",
    "    api_call = call_with_rate_limit if rate_limit else call_claude\n",
    "    context = row.get('context', '')\n",
    "    \n",
    "    if level == 5:\n",
    "        prompt1 = get_moralchoice_prompt(5, context, row['option_a'], row['option_b'],\n",
    "                                         include_confidence=include_confidence)\n",
    "        response1 = api_call(prompt1, thinking)\n",
    "        \n",
    "        prompt2 = get_moralchoice_prompt(5, context, row['option_a'], row['option_b'],\n",
    "                                         previous_response=response1.content,\n",
    "                                         include_confidence=include_confidence)\n",
    "        response2 = api_call(prompt2, thinking)\n",
    "        \n",
    "        content = response2.content\n",
    "        full_response = f\"[PASS1]\\n{response1.content}\\n\\n[PASS2]\\n{response2.content}\"\n",
    "        thinking_content = response2.thinking\n",
    "        input_tokens = response1.input_tokens + response2.input_tokens\n",
    "        output_tokens = response1.output_tokens + response2.output_tokens\n",
    "    else:\n",
    "        prompt = get_moralchoice_prompt(level, context, row['option_a'], row['option_b'],\n",
    "                                        include_confidence=include_confidence)\n",
    "        response = api_call(prompt, thinking)\n",
    "        \n",
    "        content = response.content\n",
    "        full_response = response.content\n",
    "        thinking_content = response.thinking\n",
    "        input_tokens = response.input_tokens\n",
    "        output_tokens = response.output_tokens\n",
    "    \n",
    "    # Extract answer and confidence\n",
    "    extraction = extract_moralchoice_with_confidence(content)\n",
    "    \n",
    "    return {\n",
    "        'item_id': row['item_id'],\n",
    "        'context': row.get('context', '')[:100],\n",
    "        'option_a': row['option_a'][:100],\n",
    "        'option_b': row['option_b'][:100],\n",
    "        'ambiguity': row.get('ambiguity', None),\n",
    "        'level': level,\n",
    "        'thinking': thinking,\n",
    "        'response': full_response,\n",
    "        'thinking_content': thinking_content,\n",
    "        'extracted_answer': extraction['answer'],\n",
    "        'confidence': extraction['confidence'],\n",
    "        'confidence_category': extraction['confidence_category'],\n",
    "        'response_length': len(full_response.split()),\n",
    "        'reasoning_markers': count_reasoning_markers(full_response),\n",
    "        'uncertainty_markers': count_uncertainty_markers(full_response),\n",
    "        'input_tokens': input_tokens,\n",
    "        'output_tokens': output_tokens,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "    }\n",
    "\n",
    "print(\"Core functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(data, dataset_type, levels, thinking_conditions, n_runs,\n",
    "                   include_confidence=True, save_checkpoints=True, checkpoint_dir=\"results/notebook\"):\n",
    "    \"\"\"\n",
    "    Run experiment on dataset with specified parameters.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with experiment items\n",
    "        dataset_type: 'ethics' or 'moralchoice'\n",
    "        levels: List of reflection levels to test\n",
    "        thinking_conditions: List of thinking conditions [False, True]\n",
    "        n_runs: Number of runs per condition\n",
    "        include_confidence: For MoralChoice, whether to ask for confidence\n",
    "        save_checkpoints: Whether to save intermediate results\n",
    "        checkpoint_dir: Directory for checkpoint files\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with all results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    if save_checkpoints:\n",
    "        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    total_conditions = len(levels) * len(thinking_conditions) * n_runs\n",
    "    condition_num = 0\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        for thinking in thinking_conditions:\n",
    "            for level in levels:\n",
    "                condition_num += 1\n",
    "                thinking_label = \"ON\" if thinking else \"OFF\"\n",
    "                \n",
    "                print(f\"\\n[{condition_num}/{total_conditions}] \"\n",
    "                      f\"{dataset_type.upper()} Run {run+1}, Level {level}, Thinking {thinking_label}\")\n",
    "                \n",
    "                for _, row in tqdm(data.iterrows(), total=len(data),\n",
    "                                   desc=f\"R{run+1}-L{level}-{thinking_label}\"):\n",
    "                    try:\n",
    "                        if dataset_type == 'ethics':\n",
    "                            result = run_single_ethics(row, level, thinking)\n",
    "                        else:\n",
    "                            result = run_single_moralchoice(row, level, thinking, include_confidence)\n",
    "                        \n",
    "                        result['run'] = run\n",
    "                        results.append(result)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error on item {row['item_id']}: {e}\")\n",
    "                        results.append({\n",
    "                            'item_id': row['item_id'],\n",
    "                            'level': level,\n",
    "                            'thinking': thinking,\n",
    "                            'run': run,\n",
    "                            'error': str(e),\n",
    "                            'timestamp': datetime.now().isoformat(),\n",
    "                        })\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if save_checkpoints:\n",
    "                    checkpoint_df = pd.DataFrame(results)\n",
    "                    checkpoint_df.to_csv(f\"{checkpoint_dir}/{dataset_type}_checkpoint.csv\", index=False)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(\"Experiment runner loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "start_time = datetime.now()\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Start time: {start_time}\")\n",
    "\n",
    "ethics_results = None\n",
    "mc_results = None\n",
    "\n",
    "if RUN_ETHICS:\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"RUNNING ETHICS EXPERIMENT\")\n",
    "    print(\"=\" * 40)\n",
    "    ethics_results = run_experiment(\n",
    "        ethics_data, 'ethics',\n",
    "        levels=LEVELS,\n",
    "        thinking_conditions=THINKING_CONDITIONS,\n",
    "        n_runs=N_RUNS,\n",
    "        save_checkpoints=SAVE_CHECKPOINTS,\n",
    "        checkpoint_dir=CHECKPOINT_DIR\n",
    "    )\n",
    "    print(f\"\\nETHICS complete: {len(ethics_results)} observations\")\n",
    "\n",
    "if RUN_MORALCHOICE:\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"RUNNING MORALCHOICE EXPERIMENT\")\n",
    "    print(\"=\" * 40)\n",
    "    mc_results = run_experiment(\n",
    "        mc_data, 'moralchoice',\n",
    "        levels=LEVELS,\n",
    "        thinking_conditions=THINKING_CONDITIONS,\n",
    "        n_runs=N_RUNS,\n",
    "        include_confidence=INCLUDE_CONFIDENCE,\n",
    "        save_checkpoints=SAVE_CHECKPOINTS,\n",
    "        checkpoint_dir=CHECKPOINT_DIR\n",
    "    )\n",
    "    print(f\"\\nMoralChoice complete: {len(mc_results)} observations\")\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Duration: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quick Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_analysis(results, dataset_type='ethics'):\n",
    "    \"\"\"Generate quick summary statistics.\"\"\"\n",
    "    if results is None or len(results) == 0:\n",
    "        print(f\"No {dataset_type} results to analyze.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{dataset_type.upper()} RESULTS SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"\\nTotal observations: {len(results)}\")\n",
    "    print(f\"Unique items: {results['item_id'].nunique()}\")\n",
    "    \n",
    "    # Check for errors\n",
    "    if 'error' in results.columns:\n",
    "        errors = results['error'].notna().sum()\n",
    "        print(f\"Errors: {errors}\")\n",
    "    \n",
    "    # Extraction success\n",
    "    if dataset_type == 'ethics':\n",
    "        extraction_col = 'extracted_answer'\n",
    "    else:\n",
    "        extraction_col = 'extracted_answer'\n",
    "    \n",
    "    if extraction_col in results.columns:\n",
    "        extraction_failures = results[extraction_col].isna().sum()\n",
    "        print(f\"Extraction failures: {extraction_failures} ({extraction_failures/len(results)*100:.1f}%)\")\n",
    "    \n",
    "    # Accuracy by condition (ETHICS)\n",
    "    if dataset_type == 'ethics' and 'correct' in results.columns:\n",
    "        print(\"\\nAccuracy by Level & Thinking:\")\n",
    "        accuracy = results.groupby(['level', 'thinking'])['correct'].mean().unstack()\n",
    "        accuracy.columns = ['Thinking OFF', 'Thinking ON']\n",
    "        print(accuracy.round(3).to_string())\n",
    "    \n",
    "    # Answer distribution (MoralChoice)\n",
    "    if dataset_type == 'moralchoice' and 'extracted_answer' in results.columns:\n",
    "        print(\"\\nAnswer Distribution by Level:\")\n",
    "        answer_dist = results.groupby('level')['extracted_answer'].value_counts(normalize=True).unstack()\n",
    "        print(answer_dist.round(3).to_string())\n",
    "        \n",
    "        if 'confidence' in results.columns:\n",
    "            print(\"\\nMean Confidence by Level:\")\n",
    "            conf = results.groupby(['level', 'thinking'])['confidence'].mean().unstack()\n",
    "            conf.columns = ['Thinking OFF', 'Thinking ON']\n",
    "            print(conf.round(1).to_string())\n",
    "    \n",
    "    # Token usage\n",
    "    if 'input_tokens' in results.columns:\n",
    "        print(\"\\nToken Usage by Level:\")\n",
    "        tokens = results.groupby('level').agg({\n",
    "            'input_tokens': 'mean',\n",
    "            'output_tokens': 'mean'\n",
    "        }).round(0)\n",
    "        print(tokens.to_string())\n",
    "\n",
    "# Run quick analysis\n",
    "if ethics_results is not None:\n",
    "    quick_analysis(ethics_results, 'ethics')\n",
    "\n",
    "if mc_results is not None:\n",
    "    quick_analysis(mc_results, 'moralchoice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, dataset_type='ethics'):\n",
    "    \"\"\"Generate visualization of results.\"\"\"\n",
    "    if results is None or len(results) == 0:\n",
    "        print(f\"No {dataset_type} results to plot.\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Main metric by level and thinking\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    if dataset_type == 'ethics' and 'correct' in results.columns:\n",
    "        # Accuracy plot\n",
    "        accuracy = results.groupby(['level', 'thinking'])['correct'].mean().reset_index()\n",
    "        accuracy['thinking_label'] = accuracy['thinking'].map({True: 'Thinking ON', False: 'Thinking OFF'})\n",
    "        \n",
    "        sns.lineplot(data=accuracy, x='level', y='correct', hue='thinking_label',\n",
    "                     marker='o', markersize=10, ax=ax1)\n",
    "        ax1.set_xlabel('Reflection Level')\n",
    "        ax1.set_ylabel('Accuracy')\n",
    "        ax1.set_title(f'{dataset_type.upper()}: Accuracy by Level')\n",
    "        ax1.set_ylim(0, 1.05)\n",
    "        ax1.legend(title='')\n",
    "    \n",
    "    elif dataset_type == 'moralchoice' and 'extracted_answer' in results.columns:\n",
    "        # Preference plot (% choosing A)\n",
    "        results_clean = results[results['extracted_answer'].isin(['A', 'B'])].copy()\n",
    "        results_clean['chose_A'] = (results_clean['extracted_answer'] == 'A').astype(int)\n",
    "        \n",
    "        pref = results_clean.groupby(['level', 'thinking'])['chose_A'].mean().reset_index()\n",
    "        pref['thinking_label'] = pref['thinking'].map({True: 'Thinking ON', False: 'Thinking OFF'})\n",
    "        \n",
    "        sns.lineplot(data=pref, x='level', y='chose_A', hue='thinking_label',\n",
    "                     marker='o', markersize=10, ax=ax1)\n",
    "        ax1.set_xlabel('Reflection Level')\n",
    "        ax1.set_ylabel('Proportion Choosing A')\n",
    "        ax1.set_title(f'{dataset_type.upper()}: Preference by Level')\n",
    "        ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax1.set_ylim(0, 1)\n",
    "        ax1.legend(title='')\n",
    "    \n",
    "    # Plot 2: Response length by level\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    if 'response_length' in results.columns:\n",
    "        length = results.groupby(['level', 'thinking'])['response_length'].mean().reset_index()\n",
    "        length['thinking_label'] = length['thinking'].map({True: 'Thinking ON', False: 'Thinking OFF'})\n",
    "        \n",
    "        sns.barplot(data=length, x='level', y='response_length', hue='thinking_label', ax=ax2)\n",
    "        ax2.set_xlabel('Reflection Level')\n",
    "        ax2.set_ylabel('Mean Response Length (words)')\n",
    "        ax2.set_title('Response Length by Level')\n",
    "        ax2.legend(title='')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots\n",
    "if ethics_results is not None:\n",
    "    plot_results(ethics_results, 'ethics')\n",
    "\n",
    "if mc_results is not None:\n",
    "    plot_results(mc_results, 'moralchoice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = Path(\"results/notebook\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if ethics_results is not None:\n",
    "    output_path = output_dir / f\"ethics_results_{timestamp}.csv\"\n",
    "    ethics_results.to_csv(output_path, index=False)\n",
    "    print(f\"ETHICS results saved to: {output_path}\")\n",
    "\n",
    "if mc_results is not None:\n",
    "    output_path = output_dir / f\"moralchoice_results_{timestamp}.csv\"\n",
    "    mc_results.to_csv(output_path, index=False)\n",
    "    print(f\"MoralChoice results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Single Item Testing\n",
    "\n",
    "Test individual items to debug or explore responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_item(item_idx=0, dataset='ethics', level=2, thinking=False, show_response=True):\n",
    "    \"\"\"\n",
    "    Test a single item for debugging.\n",
    "    \n",
    "    Args:\n",
    "        item_idx: Index of item in dataset\n",
    "        dataset: 'ethics' or 'moralchoice'\n",
    "        level: Reflection level\n",
    "        thinking: Enable extended thinking\n",
    "        show_response: Print full response\n",
    "    \"\"\"\n",
    "    if dataset == 'ethics':\n",
    "        data = load_ethics_data(None)  # Load full for selection\n",
    "        row = data.iloc[item_idx]\n",
    "        result = run_single_ethics(row, level, thinking, rate_limit=False)\n",
    "    else:\n",
    "        data = load_moralchoice_data(None)\n",
    "        row = data.iloc[item_idx]\n",
    "        result = run_single_moralchoice(row, level, thinking, INCLUDE_CONFIDENCE, rate_limit=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SINGLE ITEM TEST - {dataset.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Item ID: {result['item_id']}\")\n",
    "    print(f\"Level: {level}, Thinking: {'ON' if thinking else 'OFF'}\")\n",
    "    \n",
    "    if dataset == 'ethics':\n",
    "        print(f\"\\nScenario: {row['scenario'][:200]}...\")\n",
    "        print(f\"\\nCorrect: {result['correct_answer']}\")\n",
    "        print(f\"Extracted: {result['extracted_answer']}\")\n",
    "        print(f\"Correct: {result['correct']}\")\n",
    "    else:\n",
    "        print(f\"\\nContext: {row.get('context', 'N/A')[:100]}\")\n",
    "        print(f\"Option A: {row['option_a'][:100]}\")\n",
    "        print(f\"Option B: {row['option_b'][:100]}\")\n",
    "        print(f\"\\nExtracted Answer: {result['extracted_answer']}\")\n",
    "        print(f\"Confidence: {result.get('confidence', 'N/A')}\")\n",
    "    \n",
    "    print(f\"\\nTokens: {result['input_tokens']} in, {result['output_tokens']} out\")\n",
    "    print(f\"Response length: {result['response_length']} words\")\n",
    "    \n",
    "    if show_response:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"FULL RESPONSE:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(result['response'])\n",
    "        \n",
    "        if result.get('thinking_content'):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(\"THINKING CONTENT:\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(result['thinking_content'][:1000] + \"...\" if len(result['thinking_content']) > 1000 else result['thinking_content'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example: Test item 0 at level 2 with thinking OFF\n",
    "# Uncomment to run:\n",
    "# test_result = test_single_item(item_idx=0, dataset='ethics', level=2, thinking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Custom Prompt Testing\n",
    "\n",
    "Test custom prompts or modified scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_custom_prompt(prompt, thinking=False):\n",
    "    \"\"\"\n",
    "    Test a custom prompt directly.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Custom prompt string\n",
    "        thinking: Enable extended thinking\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CUSTOM PROMPT TEST\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Thinking: {'ON' if thinking else 'OFF'}\")\n",
    "    print(f\"\\nPrompt:\\n{prompt}\")\n",
    "    \n",
    "    response = call_claude(prompt, thinking)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"RESPONSE:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(response.content)\n",
    "    \n",
    "    if response.thinking:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"THINKING:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(response.thinking[:500] + \"...\" if len(response.thinking) > 500 else response.thinking)\n",
    "    \n",
    "    print(f\"\\nTokens: {response.input_tokens} in, {response.output_tokens} out\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example custom prompt test:\n",
    "# custom_response = test_custom_prompt(\n",
    "#     \"\"\"Scenario: A person finds $100 on the ground with no owner in sight.\n",
    "#     \n",
    "#     Is keeping the money wrong or not wrong? Think step by step.\"\"\",\n",
    "#     thinking=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Previous Results\n",
    "\n",
    "Load and analyze results from previous runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_previous_results(path=None, dataset_type='ethics'):\n",
    "    \"\"\"\n",
    "    Load results from a previous experiment run.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to CSV file (or None to use default)\n",
    "        dataset_type: 'ethics' or 'moralchoice'\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        path = f\"results/processed/{dataset_type}_results.csv\"\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"File not found: {path}\")\n",
    "        return None\n",
    "    \n",
    "    results = pd.read_csv(path)\n",
    "    print(f\"Loaded {len(results)} observations from {path}\")\n",
    "    print(f\"Levels: {sorted(results['level'].unique())}\")\n",
    "    print(f\"Thinking conditions: {results['thinking'].unique()}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load main experiment results:\n",
    "# prev_ethics = load_previous_results(dataset_type='ethics')\n",
    "# prev_mc = load_previous_results(dataset_type='moralchoice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Reflection Levels\n",
    "| Level | Name | Description |\n",
    "|-------|------|-------------|\n",
    "| 0 | Direct | One-word answer only |\n",
    "| 1 | Minimal | Simple question |\n",
    "| 2 | CoT | Step-by-step reasoning |\n",
    "| 3 | Structured | Principle-based scaffolding |\n",
    "| 4 | Adversarial | Consider counterarguments |\n",
    "| 5 | Two-Pass | Initial + balanced reflection |\n",
    "\n",
    "### Key Functions\n",
    "- `run_experiment()` - Run full experiment with parameters\n",
    "- `test_single_item()` - Test one item for debugging\n",
    "- `test_custom_prompt()` - Test any custom prompt\n",
    "- `quick_analysis()` - Generate summary stats\n",
    "- `plot_results()` - Visualize results\n",
    "- `load_previous_results()` - Load saved results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
